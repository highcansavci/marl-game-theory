# Nash Dengesi'ni Ã–ÄŸrenmek: Q-Learning von Neumann ile BuluÅŸuyor

---

## GiriÅŸ: Stratejiden Dengeye GeÃ§iÅŸ
Ã–nceki yazÄ±mÄ±zda AlphaGo'dan StarCraft'a, Ã§ok ajanlÄ± sistemlerde evrimsel stratejilerin anatomisini inceledik. AlphaZero'nun self-play mekanizmasÄ±yla Ã¶ÄŸrendiÄŸi taktikler, ÅŸimdi Nash dengesi gibi klasik oyun teorisi kavramlarÄ±yla kesiÅŸiyorâ€Š-â€Švon Neumann'Ä±n minimax felsefesi, Q-learning'in stokastik dÃ¼nyasÄ±nda nasÄ±l evriliyor?

Bu yazÄ±da, Ã§ok ajanlÄ± pekiÅŸtirmeli Ã¶ÄŸrenmenin (MARL) en Ã¶nemli yapÄ± taÅŸlarÄ±ndan biri olan **Nash-Q Learning** algoritmasÄ±nÄ± derinlemesine inceleyeceÄŸiz. Nash-Q Learning, klasik Q-learning'in Ã§ok ajanlÄ± uzantÄ±sÄ±dÄ±r ve iÅŸbirlikÃ§i olmayan oyunlarda Nash dengesini Ã¶ÄŸrenerek optimal politikalar bulmayÄ± hedefler. Stokastik oyunlarda denge hesaplama zorluklarÄ± ve bu algoritmanÄ±n neden MARL'da devrimsel olduÄŸunu keÅŸfedeceÄŸiz.

GÃ¼nÃ¼mÃ¼z yapay zeka uygulamalarÄ±ndaâ€Š-â€Šotonom araÃ§lardan siber gÃ¼venliÄŸe, drone sÃ¼rÃ¼lerinden enerji aÄŸÄ± yÃ¶netimine kadarâ€Š-â€Šajanlar arasÄ± Ã§atÄ±ÅŸma ve iÅŸbirliÄŸi simÃ¼lasyonu kritik Ã¶neme sahip. Nash-Q Learning, bu karmaÅŸÄ±k etkileÅŸimleri modellemek ve optimal stratejiler geliÅŸtirmek iÃ§in gÃ¼Ã§lÃ¼ bir araÃ§ sunuyor.

---

## Teorik Temel: Oyun Teorisi ve MARL'in BuluÅŸmasÄ±

### Von Neumann'dan Nash'e: Oyun Teorisinin Temelleri
John von Neumann'Ä±n 1928'de ortaya koyduÄŸu **minimax teoremi**, oyun teorisinin temel taÅŸlarÄ±ndan biridir. Bu teorem, sÄ±fÄ±r toplamlÄ± oyunlarda (zero-sum games) her oyuncunun garantili bir kazanÃ§ seviyesi olduÄŸunu matematiksel olarak kanÄ±tlar. SÄ±fÄ±r toplamlÄ± oyunlarda bir oyuncunun kazancÄ±, diÄŸerinin kaybÄ±na eÅŸittirâ€Š-â€ŠsatranÃ§, poker (iki oyunculu) ve go bu kategoriye girer.

Von Neumann'Ä±n minimax yaklaÅŸÄ±mÄ± ÅŸÃ¶yle Ã§alÄ±ÅŸÄ±r: Bir oyuncu, rakibinin en kÃ¶tÃ¼ hamleyi yapacaÄŸÄ±nÄ± varsayarak kendi maksimum kazancÄ±nÄ± garanti eden stratejiyi seÃ§er. Matematiksel olarak, bu "rakibin maksimum zararÄ±nÄ± minimize etmek" anlamÄ±na gelir. Bu yaklaÅŸÄ±m, deterministik ve iki oyunculu oyunlarda gÃ¼Ã§lÃ¼ sonuÃ§lar verirken, gerÃ§ek dÃ¼nya senaryolarÄ±nÄ±n Ã§oÄŸu iÃ§in yetersiz kalÄ±r.

1950'de John Nash, oyun teorisine devrim niteliÄŸinde bir katkÄ± yaptÄ±: **Nash Dengesi**. Nash dengesi, sÄ±fÄ±r toplamlÄ± olmayan ve iki oyuncudan fazla ajanlÄ± oyunlarda bile Ã§alÄ±ÅŸan genel bir Ã§Ã¶zÃ¼m kavramÄ±dÄ±r. Bir strateji profili Nash dengesinde ise, hiÃ§bir ajan tek baÅŸÄ±na stratejisini deÄŸiÅŸtirerek daha fazla kazanÃ§ elde edemezâ€Š-â€Šher ajanÄ±n stratejisi, diÄŸerlerinin stratejilerine gÃ¶re en iyi yanÄ±ttÄ±r (best response).

Matematiksel olarak ifade edersek: Ajan i'nin stratejisi, diÄŸer tÃ¼m ajanlarÄ±n stratejilerine gÃ¶re kendi faydasÄ±nÄ± maksimize ediyorsa Nash dengesindeyiz. Bu denge noktasÄ±, Ã§ok ajanlÄ± sistemlerde "istikrarlÄ±" stratejileri tanÄ±mlar.

**Stokastik Oyunlar** ise Markov Decision Process'in (MDP) Ã§ok ajanlÄ± versiyonudur. Lloyd Shapley tarafÄ±ndan 1953'te tanÄ±mlanan bu model, durumlar, aksiyonlar, Ã¶dÃ¼ller ve geÃ§iÅŸ olasÄ±lÄ±klarÄ±ndan oluÅŸur. Tek ajanlÄ± MDP'lerden farkÄ±, her durumda birden fazla ajanÄ±n eÅŸ zamanlÄ± aksiyon aldÄ±ÄŸÄ± ve sistem dinamiklerinin tÃ¼m ajanlarÄ±n aksiyonlarÄ±na baÄŸlÄ± olmasÄ±dÄ±r.

StarCraft'taki strateji evriminde gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi, dinamik ortamlar stokastik oyunlara benzerâ€Š-â€Šburada Nash dengesi, evrimi "istikrar" ile sÄ±nÄ±rlÄ±yor. AlphaZero'nun self-play sÃ¼recinde keÅŸfettiÄŸi stratejiler, aslÄ±nda Nash dengesine yakÄ±nsamanÄ±n bir Ã¶rneÄŸidir.

### Q-Learning HatÄ±rlatmasÄ± ve MARL ZorluklarÄ±
Klasik **Q-Learning**, tek ajanlÄ± pekiÅŸtirmeli Ã¶ÄŸrenmenin temel algoritmalarÄ±ndan biridir. Watkins ve Dayan'Ä±n 1992'de geliÅŸtirdiÄŸi bu algoritma, bir ajanÄ±n durum-aksiyon Ã§iftlerinin deÄŸerini (Q-deÄŸerlerini) Ã¶ÄŸrenmesini saÄŸlar. Bellman denklemine dayanan gÃ¼ncelleme kuralÄ± oldukÃ§a basittir: Her adÄ±mda, ajan mevcut Q-deÄŸerini, aldÄ±ÄŸÄ± Ã¶dÃ¼l ve gelecekteki en iyi aksiyonun tahmini deÄŸerine gÃ¶re gÃ¼nceller.

Tek ajanlÄ± ortamlar iÃ§in son derece baÅŸarÄ±lÄ± olan Q-Learning, Ã§ok ajanlÄ± ortamlara geÃ§tiÄŸimizde ciddi zorluklarla karÅŸÄ±laÅŸÄ±r:
*   ***Non-stationarity* (DuraÄŸan Olmama):** Tek ajanlÄ± Ã¶ÄŸrenmede ortam sabit bir dinamiÄŸe sahiptirâ€Š-â€ŠaynÄ± durumda aynÄ± aksiyonu alÄ±rsanÄ±z, beklenen sonuÃ§ deÄŸiÅŸmez. Ancak Ã§ok ajanlÄ± sistemlerde, diÄŸer ajanlar da Ã¶ÄŸreniyor ve stratejilerini deÄŸiÅŸtiriyor. Bu, ortamÄ± sÃ¼rekli deÄŸiÅŸen, duraÄŸan olmayan bir hale getirir. Klasik Q-Learning'in konverjans garantileri bu koÅŸulda geÃ§erliliÄŸini yitirir.
*   ***Curse of Dimensionality* (Boyut Laneti):** N ajan varsa ve her ajan M aksiyona sahipse, ortak aksiyon uzayÄ± M Ã¼zeri N boyutunda olur. Bu eksponansiyel bÃ¼yÃ¼me, Q-tablolarÄ±nÄ± veya derin aÄŸlarÄ± Ã§ok bÃ¼yÃ¼k hale getirir ve Ã¶ÄŸrenmeyi neredeyse imkansÄ±z kÄ±lar.
*   **Kredi Atama Problemi:** Bir Ã¶dÃ¼l aldÄ±ÄŸÄ±nÄ±zda, bu hangi ajanÄ±n katkÄ±sÄ±ndan kaynaklanÄ±yor? DiÄŸer ajanlarÄ±n aksiyonlarÄ± sizin baÅŸarÄ±nÄ±zÄ± nasÄ±l etkiliyor? Bu sorularÄ±n cevabÄ±, Ã§ok ajanlÄ± Ã¶ÄŸrenmede temel bir zorluktur.

2025 ICML konferansÄ±nda yayÄ±nlanan *"Multi-Agent Reinforcement Learning in Games"* baÅŸlÄ±klÄ± derleme makalesi, MARL ve oyun teorisi konverjansÄ±nÄ± kapsamlÄ± ÅŸekilde inceliyor. AraÅŸtÄ±rmacÄ±lar, Nash dengesinin duraÄŸan olmayan ortamlara uyarlanmasÄ±nÄ±n ve derin Ã¶ÄŸrenme ile entegrasyonunun, gelecek yÄ±llarda MARL'Ä±n en Ã¶nemli araÅŸtÄ±rma alanlarÄ±ndan biri olacaÄŸÄ±nÄ± vurguluyor.

---

## Nash-Q Learning AlgoritmasÄ±: DetaylÄ± Analizi
Nash-Q Learning, Junling Hu ve Michael Wellman'Ä±n 1998'de geliÅŸtirdiÄŸi Ã¶ncÃ¼ bir algoritmadÄ±r. Temel fikir ÅŸudur: Her ajan, sadece kendi Q-deÄŸerlerini deÄŸil, Nash dengesi koÅŸulunu saÄŸlayan Q-deÄŸerlerini Ã¶ÄŸrenir. Bu, klasik Q-Learning'in doÄŸrudan Ã§ok ajanlÄ± uzantÄ±sÄ± deÄŸil, oyun teorisi ile pekiÅŸtirmeli Ã¶ÄŸrenmenin entegrasyonudur.

AlgoritmanÄ±n Ã§alÄ±ÅŸma prensibi ÅŸu adÄ±mlardan oluÅŸur:
1.  **BaÅŸlangÄ±Ã§:** Her ajan iÃ§in Q-deÄŸerleri rastgele veya sÄ±fÄ±r olarak baÅŸlatÄ±lÄ±r. Her ajan kendi Q-tablosunu tutarâ€Š-â€Šbu tablo, her durumda her olasÄ± ortak aksiyonun deÄŸerini saklar.
2.  **Best-Response Hesaplama:** Her iterasyonda, her ajan diÄŸer ajanlarÄ±n mevcut politikalarÄ±na karÅŸÄ± en iyi yanÄ±tÄ±nÄ± (best response) hesaplar. Bu, "diÄŸer ajanlar ÅŸu stratejiyi oynarsa, ben ne yapmalÄ±yÄ±m?" sorusuna verilen cevaptÄ±r. Best-response Q-deÄŸeri, ajanÄ±n tek baÅŸÄ±na optimize edebileceÄŸi aksiyonun deÄŸeridir.
3.  **Nash Q-DeÄŸeri GÃ¼ncelleme:** Kritik adÄ±m burada gerÃ§ekleÅŸir. Her ajan, sadece kendi best-response'una deÄŸil, tÃ¼m ajanlarÄ±n birlikte Nash dengesini oluÅŸturan strateji profiline gÃ¶re Q-deÄŸerini gÃ¼nceller. Bu, diÄŸer ajanlarÄ±n Nash politikalarÄ±na gÃ¶re beklenen deÄŸer hesaplamasÄ±dÄ±r.
4.  **Politika Ã‡Ä±karma:** GÃ¼ncellenmiÅŸ Nash Q-deÄŸerlerinden, her ajan kendi politikasÄ±nÄ± tÃ¼retir. Bu politika, o durumda Nash dengesini saÄŸlayan aksiyonu seÃ§er.
5.  **Konverjans Garantisi:** Hu ve Wellman, algoritmanÄ±n sÄ±fÄ±r toplamlÄ± stokastik oyunlarda Nash dengesine yakÄ±nsadÄ±ÄŸÄ±nÄ± matematiksel olarak kanÄ±tladÄ±. Bu, belirli koÅŸullar altÄ±nda (Ã¶ÄŸrenme oranÄ±nÄ±n yeterince kÃ¼Ã§Ã¼k olmasÄ±, sonsuz keÅŸif gibi) geÃ§erlidir.

AlgoritmanÄ±n baÅŸarÄ±sÄ±, stokastik oyunlarda denge hesaplama yeteneÄŸine dayanÄ±r. Michael Littman'Ä±n deÄŸer iterasyonu benzeri yaklaÅŸÄ±mÄ±, Nash-Q Learning'e entegre edilir: Her durumda, tÃ¼m ajanlar iÃ§in eÅŸ zamanlÄ± best-response hesaplanÄ±r ve Nash dengesine karÅŸÄ±lÄ±k gelen sabit nokta bulunur. Bu hesaplama, kÃ¼Ã§Ã¼k oyunlarda lineer programlama ile, bÃ¼yÃ¼k oyunlarda ise yaklaÅŸÄ±k yÃ¶ntemlerle yapÄ±lÄ±r.

GÃ¼ncel araÅŸtÄ±rmalar, Nash-Q Learning'i derin Ã¶ÄŸrenme ile birleÅŸtirerek Ã¶lÃ§eklenebilirliÄŸini artÄ±rÄ±yor. 2025 yÄ±lÄ±nda arXiv'de yayÄ±nlanan *"Nash Q-Network for Multi-Agent Cybersecurity Simulation"* makalesi, derin sinir aÄŸlarÄ±yla Nash Q-deÄŸerlerinin Ã¶ÄŸrenildiÄŸi bir yaklaÅŸÄ±m sunuyor. Siber savunma simÃ¼lasyonlarÄ±nda, bu yÃ¶ntem konverjans hÄ±zÄ±nÄ± %30 oranÄ±nda artÄ±rmÄ±ÅŸ ve gerÃ§ek zamanlÄ± tehditlere karÅŸÄ± daha saÄŸlam politikalar Ã¼retmiÅŸtir.

BaÅŸka bir ilgi Ã§ekici Ã§alÄ±ÅŸma, *"Multi-Agent Nash Q-Learning for Node Security"* baÅŸlÄ±klÄ± makale. Bu araÅŸtÄ±rma, kiÅŸisel veri gizliliÄŸinde rakip ajanlar arasÄ± dengeyi inceliyor. SaldÄ±rgan ajanlar kullanÄ±cÄ± verilerini Ã§almaya Ã§alÄ±ÅŸÄ±rken, savunmacÄ± ajanlar gizliliÄŸi korumaya Ã§alÄ±ÅŸÄ±yorâ€Š-â€ŠNash-Q Learning, bu Ã§atÄ±ÅŸmada dengeli bir gÃ¼venlik politikasÄ± Ã¶ÄŸreniyor.

### Matematiksel Ã–rnek: 2-AjanlÄ± Grid-World
Basit bir Ã¶rnek Ã¼zerinden Nash-Q Learning'in nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± inceleyelim. Ä°ki ajan (A ve B) 3x3'lÃ¼k bir Ä±zgarada hareket ediyor. Her ajan yukarÄ±, aÅŸaÄŸÄ±, sola veya saÄŸa gidebilir. Hedefteki bir kaynaÄŸa ilk ulaÅŸan ajan +10 Ã¶dÃ¼l alÄ±yor, diÄŸeri -5 Ã¶dÃ¼l alÄ±yor. EÄŸer aynÄ± anda ulaÅŸÄ±rlarsa her ikisi de +5 alÄ±yor.

BaÅŸlangÄ±Ã§ta, her ajanÄ±n Q-tablosu rastgele deÄŸerlerle dolu. Ä°lk episode'da, ajanlar epsilon-greedy stratejiyle rastgele hareket ediyor. Ajan A hedefe ulaÅŸÄ±yor ve +10 Ã¶dÃ¼l alÄ±yor. Åimdi gÃ¼ncelleme:

*   **Ajan A iÃ§in best-response:** Mevcut durumda, B'nin en olasÄ± hamlesi gÃ¶z Ã¶nÃ¼ne alÄ±narak A'nÄ±n en iyi aksiyonu hesaplanÄ±r.
*   **Nash Q-deÄŸeri:** A ve B'nin birlikte Nash dengesini saÄŸlayan aksiyon profili belirlenir (Ã¶rneÄŸin, A saÄŸa git, B yukarÄ± git).
*   **Q-tablosu gÃ¼ncellenir:** Bellman benzeri gÃ¼ncelleme, ancak max operatÃ¶rÃ¼ yerine Nash dengesi deÄŸeri kullanÄ±lÄ±r.

1000 episode sonunda, Q-tablolarÄ± yakÄ±nsar. Her ajan, diÄŸerinin stratejisini gÃ¶z Ã¶nÃ¼ne alarak optimal rotayÄ± Ã¶ÄŸrenmiÅŸ olur. Ä°lginÃ§ olan, Nash dengesinde bazen ajanlarÄ±n kaynaÄŸa aynÄ± anda ulaÅŸmayÄ± tercih etmesidirâ€Š-â€ŠÃ§Ã¼nkÃ¼ bu, rekabetÃ§i bir yarÄ±ÅŸ riskini azaltÄ±r ve garantili +5 Ã¶dÃ¼lÃ¼ saÄŸlar.

---

## Kod Ã–rneÄŸi: Python ile Nash-Q Learning Implementasyonu

### Nash Dengesi iÃ§in Lineer Programlama: Neyi Optimize Ediyoruz?
**Temel Soru: Rakip Stratejisini Bilmiyoruz!**

Nash-Q Learning'de her ajan, rakibinin ne yapacaÄŸÄ±nÄ± tam olarak bilmiyor. Rakip de Ã¶ÄŸreniyor ve stratejisini sÃ¼rekli deÄŸiÅŸtiriyor. Peki bu durumda "en iyi strateji" ne demek?

Ä°ÅŸte burada minimax dÃ¼ÅŸÃ¼ncesi devreye giriyor: "En kÃ¶tÃ¼ duruma karÅŸÄ± en iyi hazÄ±rlÄ±k yap."

Yani LP ile optimize ettiÄŸimiz ÅŸey:
> "Rakip benim stratejimi bilse ve bana karÅŸÄ± en kÃ¶tÃ¼ hamleyi yapsa bile, ben minimum ne kadar kazanÃ§ garanti edebilirim? Bu minimum deÄŸeri maksimize et."

Bu yaklaÅŸÄ±m, oyun teorisinin kalbinde yatan gÃ¼venlik prensibi: Kendini en kÃ¶tÃ¼ senaryoya gÃ¶re hazÄ±rla, o zaman hiÃ§ sÃ¼rpriz yeme. AÅŸaÄŸÄ±da LP ile neyi optimize ettiÄŸimizi detaylÄ± olarak gÃ¶rebilirsiniz.

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.optimize import linprog

class StochasticRPS:
    """Stokastik TaÅŸ-KaÄŸÄ±t-Makas Oyunu"""
    def __init__(self):
        self.actions = ['rock', 'paper', 'scissors']
        self.n_actions = 3
        
    def get_reward(self, action1, action2):
        """Ä°ki ajanÄ±n aksiyonlarÄ±na gÃ¶re stokastik Ã¶dÃ¼l dÃ¶ndÃ¼rÃ¼r"""
        # Beraberlik
        if action1 == action2:
            return 0.0, 0.0
        
        # Kazanan belirleme (rock=0, paper=1, scissors=2)
        if (action1 == 0 and action2 == 2) or \
           (action1 == 1 and action2 == 0) or \
           (action1 == 2 and action2 == 1):
            # Ajan 1 kazandÄ±
            reward = np.random.uniform(0.8, 1.2)
            return reward, -reward
        else:
            # Ajan 2 kazandÄ±
            reward = np.random.uniform(0.8, 1.2)
            return -reward, reward

class NashQLearner:
    """Nash-Q Learning AjanÄ± (Lineer Programlama ile Nash Dengesi)"""
    def __init__(self, n_actions, learning_rate=0.05, discount=0.9):
        self.n_actions = n_actions
        self.alpha = learning_rate
        self.gamma = discount
        
        # Q-tablosu: Q[my_action, opp_action]
        self.Q = np.zeros((n_actions, n_actions))
        
    def compute_nash_equilibrium(self):
        """
        Lineer Programlama ile Nash dengesini hesaplar.
        
        Maksimize etmek istediÄŸimiz:
        V = min_j Î£_i (Ï€_i Ã— Q[i,j])
        
        LP FormÃ¼lasyonu:
        maximize: V
        subject to:
        Î£_i (Ï€_i Ã— Q[i,j]) >= V  for all j (her rakip aksiyonu iÃ§in)
        Î£_i Ï€_i = 1
        Ï€_i >= 0
        """
        n = self.n_actions
        
        # Q-matrisinin transpozunu al (rakip aksiyonlarÄ± iÃ§in constraint'ler)
        Q_T = self.Q.T
        
        # LP deÄŸiÅŸkenleri: [Ï€_0, Ï€_1, Ï€_2, V]
        # Minimize etmek iÃ§in: -V (linprog minimize eder, biz maximize istiyoruz)
        c = np.zeros(n + 1)
        c[-1] = -1  # V'yi maksimize et (minimize -V)
        
        # EÅŸitsizlik constraint'leri: -Î£(Ï€_i Ã— Q[i,j]) + V <= 0
        # Yani: V <= Î£(Ï€_i Ã— Q[i,j]) for all j
        A_ub = np.zeros((n, n + 1))
        for j in range(n):
            A_ub[j, :n] = -self.Q[:, j]  # -Q kolon j
            A_ub[j, n] = 1  # +V
        b_ub = np.zeros(n)
        
        # EÅŸitlik constraint'i: Î£ Ï€_i = 1
        A_eq = np.zeros((1, n + 1))
        A_eq[0, :n] = 1
        b_eq = np.array([1.0])
        
        # Bounds: Ï€_i >= 0, V unbounded
        bounds = [(0, 1) for _ in range(n)] + [(None, None)]
        
        try:
            # LP Ã§Ã¶zÃ¼mÃ¼
            result = linprog(c, A_ub=A_ub, b_ub=b_ub, 
                           A_eq=A_eq, b_eq=b_eq, 
                           bounds=bounds, method='highs')
            
            if result.success:
                nash_policy = result.x[:n]
                # Normalize et (kÃ¼Ã§Ã¼k sayÄ±sal hatalar iÃ§in)
                nash_policy = np.maximum(nash_policy, 0)
                nash_policy /= nash_policy.sum()
                return nash_policy
            else:
                # LP baÅŸarÄ±sÄ±z olursa uniform politika dÃ¶ndÃ¼r
                return np.ones(n) / n
                
        except Exception as e:
            # Hata durumunda uniform politika
            print(f"LP hatasÄ±: {e}")
            return np.ones(n) / n
    
    def get_value(self, opponent_policy):
        """Rakibin politikasÄ±na gÃ¶re bu ajanÄ±n Nash deÄŸerini hesaplar"""
        # Her aksiyonun rakip politikasÄ±na gÃ¶re beklenen deÄŸeri
        expected_q = self.Q @ opponent_policy
        return np.max(expected_q)
    
    def update(self, my_action, opp_action, reward, opp_next_policy):
        """Nash Q-deÄŸerlerini gÃ¼nceller"""
        # Gelecekteki Nash deÄŸeri
        future_value = self.get_value(opp_next_policy)
        
        # TD target
        target = reward + self.gamma * future_value
        
        # TD error ve gÃ¼ncelleme
        td_error = target - self.Q[my_action, opp_action]
        self.Q[my_action, opp_action] += self.alpha * td_error

def select_action(policy, epsilon):
    """Epsilon-greedy aksiyon seÃ§imi"""
    if np.random.random() < epsilon:
        return np.random.randint(len(policy))
    else:
        return np.random.choice(len(policy), p=policy)

def train_nash_q(n_episodes=5000, epsilon_start=0.5, epsilon_end=0.05, epsilon_decay=0.995):
    """Nash-Q Learning eÄŸitimi"""
    env = StochasticRPS()
    agent1 = NashQLearner(env.n_actions, learning_rate=0.05)
    agent2 = NashQLearner(env.n_actions, learning_rate=0.05)
    
    rewards_history = []
    epsilon = epsilon_start
    
    for episode in range(n_episodes):
        # Mevcut Nash politikalarÄ± (LP ile)
        policy1 = agent1.compute_nash_equilibrium()
        policy2 = agent2.compute_nash_equilibrium()
        
        # Epsilon-greedy aksiyon seÃ§imi
        action1 = select_action(policy1, epsilon)
        action2 = select_action(policy2, epsilon)
        
        # Ã–dÃ¼l alma
        reward1, reward2 = env.get_reward(action1, action2)
        
        # Nash Q-gÃ¼ncelleme
        agent1.update(action1, action2, reward1, policy2)
        agent2.update(action2, action1, reward2, policy1)
        
        # KayÄ±t
        rewards_history.append((reward1, reward2))
        
        # Epsilon decay
        epsilon = max(epsilon_end, epsilon * epsilon_decay)
        
        # Ä°lerleme raporu
        if (episode + 1) % 1000 == 0:
            avg_r1 = np.mean([r[0] for r in rewards_history[-1000:]])
            avg_r2 = np.mean([r[1] for r in rewards_history[-1000:]])
            print(f"Episode {episode+1}: Avg R1={avg_r1:.3f}, R2={avg_r2:.3f}, Îµ={epsilon:.3f}")
            print(f"  Agent 1 Nash: {policy1}")
            print(f"  Agent 2 Nash: {policy2}")
    
    return agent1, agent2, rewards_history

def visualize_results(agent1, agent2, history):
    """SonuÃ§larÄ± gÃ¶rselleÅŸtirir"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. Konverjans Plotu
    ax1 = axes[0, 0]
    window = 100
    avg_rewards1 = [np.mean([h[0] for h in history[i:i+window]]) 
                    for i in range(0, len(history)-window, window)]
    avg_rewards2 = [np.mean([h[1] for h in history[i:i+window]]) 
                    for i in range(0, len(history)-window, window)]
    
    x = np.arange(len(avg_rewards1)) * window
    ax1.plot(x, avg_rewards1, label='Ajan 1', linewidth=2, alpha=0.8)
    ax1.plot(x, avg_rewards2, label='Ajan 2', linewidth=2, alpha=0.8)
    ax1.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='Nash Dengesi (0)')
    ax1.set_xlabel('Episode', fontsize=11)
    ax1.set_ylabel('Ortalama Ã–dÃ¼l', fontsize=11)
    ax1.set_title('Nash-Q Learning KonverjansÄ± (LP ile)', fontsize=12, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Ajan 1 Q-Matrisi
    ax2 = axes[0, 1]
    sns.heatmap(agent1.Q, annot=True, fmt='.3f', 
                xticklabels=['Rock', 'Paper', 'Scissors'],
                yticklabels=['Rock', 'Paper', 'Scissors'],
                cmap='coolwarm', center=0, ax=ax2, cbar_kws={'label': 'Q-DeÄŸer'})
    ax2.set_title('Ajan 1 Q-Matrisi', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Rakip Aksiyonu', fontsize=11)
    ax2.set_ylabel('Kendi Aksiyonu', fontsize=11)
    
    # 3. Ajan 2 Q-Matrisi
    ax3 = axes[1, 0]
    sns.heatmap(agent2.Q, annot=True, fmt='.3f',
                xticklabels=['Rock', 'Paper', 'Scissors'],
                yticklabels=['Rock', 'Paper', 'Scissors'],
                cmap='coolwarm', center=0, ax=ax3, cbar_kws={'label': 'Q-DeÄŸer'})
    ax3.set_title('Ajan 2 Q-Matrisi', fontsize=12, fontweight='bold')
    ax3.set_xlabel('Rakip Aksiyonu', fontsize=11)
    ax3.set_ylabel('Kendi Aksiyonu', fontsize=11)
    
    # 4. Nash PolitikalarÄ±
    ax4 = axes[1, 1]
    policy1 = agent1.compute_nash_equilibrium()
    policy2 = agent2.compute_nash_equilibrium()
    
    x_pos = np.arange(3)
    width = 0.35
    
    bars1 = ax4.bar(x_pos - width/2, policy1, width, label='Ajan 1 (LP)', alpha=0.8)
    bars2 = ax4.bar(x_pos + width/2, policy2, width, label='Ajan 2 (LP)', alpha=0.8)
    ax4.axhline(y=1/3, color='r', linestyle='--', alpha=0.5, label='Ä°deal (1/3)')
    
    ax4.set_xlabel('Aksiyon', fontsize=11)
    ax4.set_ylabel('OlasÄ±lÄ±k', fontsize=11)
    ax4.set_title('Nash Dengesi PolitikalarÄ± (Lineer Programlama)', fontsize=12, fontweight='bold')
    ax4.set_xticks(x_pos)
    ax4.set_xticklabels(['Rock', 'Paper', 'Scissors'])
    ax4.legend()
    ax4.grid(True, alpha=0.3, axis='y')
    ax4.set_ylim([0, 0.5])
    
    plt.tight_layout()
    plt.show()
    plt.savefig("nash_q_learning_analysis_lp.png")

def print_analysis(agent1, agent2):
    """DetaylÄ± analiz yazdÄ±r"""
    print("\n" + "="*60)
    print("NASH-Q LEARNING ANALÄ°Z SONUÃ‡LARI (Lineer Programlama)")
    print("="*60)
    
    policy1 = agent1.compute_nash_equilibrium()
    policy2 = agent2.compute_nash_equilibrium()
    
    print("\nAjan 1 Nash PolitikasÄ± (LP):")
    print(f"  Rock:     {policy1[0]:.6f}")
    print(f"  Paper:    {policy1[1]:.6f}")
    print(f"  Scissors: {policy1[2]:.6f}")
    print(f"  Toplam:   {policy1.sum():.6f}")
    
    print("\nAjan 2 Nash PolitikasÄ± (LP):")
    print(f"  Rock:     {policy2[0]:.6f}")
    print(f"  Paper:    {policy2[1]:.6f}")
    print(f"  Scissors: {policy2[2]:.6f}")
    print(f"  Toplam:   {policy2.sum():.6f}")
    
    # Uniform mixed strategy'den sapma
    ideal = 1/3
    deviation1 = np.mean(np.abs(policy1 - ideal))
    deviation2 = np.mean(np.abs(policy2 - ideal))
    
    print(f"\nÄ°deal Nash'ten Ortalama Sapma:")
    print(f"  Ajan 1: {deviation1:.6f}")
    print(f"  Ajan 2: {deviation2:.6f}")
    
    if deviation1 < 0.01 and deviation2 < 0.01:
        print("\nâœ“ MÃ¼kemmel konverjans! LP ile tam Nash dengesi bulundu.")
    elif deviation1 < 0.05 and deviation2 < 0.05:
        print("\nâœ“ Ä°yi konverjans! Nash dengesine yakÄ±n.")
    else:
        print("\nâš  KÄ±smi konverjans. Daha fazla episode gerekebilir.")
    
    print("\nAjan 1 Q-Matrisi (SatÄ±r: Kendi, SÃ¼tun: Rakip):")
    print("        Rock    Paper   Scissors")
    for i, action in enumerate(['Rock', 'Paper', 'Scissors']):
        print(f"{action:8s}", end="")
        for j in range(3):
            print(f"{agent1.Q[i,j]:8.3f}", end="")
        print()
    
    # Minimax deÄŸeri hesapla
    v1 = agent1.get_value(policy2)
    v2 = agent2.get_value(policy1)
    print(f"\nNash DeÄŸerleri:")
    print(f"  Ajan 1 Nash Value: {v1:.4f}")
    print(f"  Ajan 2 Nash Value: {v2:.4f}")
    print(f"  Toplam: {v1 + v2:.4f} (SÄ±fÄ±ra yakÄ±n olmalÄ±)")

# Ana program
if __name__ == "__main__":
    print("Nash-Q Learning EÄŸitimi BaÅŸlÄ±yor (Lineer Programlama ile)...")
    print("Ortam: Stokastik TaÅŸ-KaÄŸÄ±t-Makas")
    print("="*60)
    
    # EÄŸitim
    agent1, agent2, history = train_nash_q(n_episodes=5000)
    
    # Analiz
    print_analysis(agent1, agent2)
    
    # GÃ¶rselleÅŸtirme
    visualize_results(agent1, agent2, history)
```

### GÃ¶rselleÅŸtirmeler
[!nash_q_learning_analysis_lp.png]
EÄŸitim sonuÃ§larÄ±nÄ± dÃ¶rt farklÄ± aÃ§Ä±dan analiz edebiliriz:

1.  **Konverjans GrafiÄŸi (Ãœst Sol - "Nash-Q Learning KonverjansÄ± LP ile"):** Bu grafik, Lineer Programlama ile Nash-Q Learning'in Ã¶ÄŸrenme dinamiÄŸini net ÅŸekilde gÃ¶steriyor. Ä°lk 2000 episode boyunca her iki ajanÄ±n ortalama Ã¶dÃ¼lleri geniÅŸ dalgalanmalar sergiliyorâ€Š-â€Šbu, epsilon-greedy stratejisinin yÃ¼ksek exploration fazÄ±. Ajan 1 ve Ajan 2'nin Ã§izgileri birbirinin neredeyse zÄ±ttÄ± gibi hareket ediyor: biri kazandÄ±ÄŸÄ±nda diÄŸeri kaybediyor. Episode 2000 civarÄ±nda dalgalanmalar belirgin ÅŸekilde azalmaya baÅŸlÄ±yor; epsilon decay sayesinde ajanlar keÅŸiften yararlanmaya (exploitation) geÃ§iyor.
    Ä°lk episode'dan sonra her iki Ã§izgi de kÄ±rmÄ±zÄ± kesikli "Nash Dengesi (0)" Ã§izgisi etrafÄ±nda dar bir bantta stabilize oluyor. Son 1500 episode'da ortalama Ã¶dÃ¼ller Â±0.05 aralÄ±ÄŸÄ±ndaâ€Š-â€Šbu, Nash dengesinin simetrik doÄŸasÄ±nÄ± yansÄ±tÄ±r. LP'nin hesapladÄ±ÄŸÄ± kesin uniform politika sayesinde, ajanlar artÄ±k birbirlerine karÅŸÄ± sistematik avantaj saÄŸlayamÄ±yor. Konverjans hem matematiksel olarak garanti edilmiÅŸ hem de empirik olarak gÃ¶zlemlenmiÅŸ.

2.  **Q-Matrisi IsÄ± HaritalarÄ± (Orta):** Her iki ajanÄ±n Q-matrisi de renk kodlu Ä±sÄ± haritasÄ±yla gÃ¶steriliyor. KÄ±rmÄ±zÄ± (pozitif) alanlar "bu aksiyon Ã§iftinde ben kazanÄ±yorum" anlamÄ±na gelirken, mavi (negatif) alanlar "bu durumda kaybediyorum" demek.
    Ajan 1'in matrisinde mÃ¼kemmel dÃ¶ngÃ¼sel patern: Rock satÄ±rÄ±nda Paper sÃ¼tunu koyu mavi (-0.994), Ã§Ã¼nkÃ¼ paper rock'u yener. AynÄ± satÄ±rda Scissors sÃ¼tunu koyu kÄ±rmÄ±zÄ± (+1.009), Ã§Ã¼nkÃ¼ rock scissors'Ä± yener. Bu patern her satÄ±rda kusursuz tekrar ediyor. LP'nin Nash dengesini kesin bir derecede hesaplayabiliyor.
    Ajan 2'nin matrisi, Ajan 1'in neredeyse kusursuz transpose'u: Rock vs Paper deÄŸeri +0.994, tam karÅŸÄ±lÄ±ÄŸÄ± -0.994. Bu matematiksel simetri, iki ajanÄ±n birbirinden tamamen baÄŸÄ±msÄ±z Ã¶ÄŸrenmesine raÄŸmen aynÄ± Nash dengesine yakÄ±nsamasÄ±nÄ± kanÄ±tlÄ±yor.

3.  **Nash PolitikalarÄ± Ã‡ubuk GrafiÄŸi (SaÄŸ Alt - "Nash Dengesi PolitikalarÄ± Lineer Programlama"):** En etkileyici sonuÃ§ burada. Her Ã¼Ã§ aksiyon iÃ§in hem Ajan 1 (mavi "LP") hem Ajan 2 (turuncu "LP") Ã§ubuklarÄ±, kÄ±rmÄ±zÄ± kesikli "Ä°deal (1/3)" Ã§izgisiyle piksel dÃ¼zeyinde hizalÄ±. Rock, Paper, Scissorsâ€Š-â€Šhepsi 0.333â€¦ deÄŸerinde. Bu, gÃ¶rsel olarak bile fark edilemeyecek kadar kesin bir uniform mixed strategy.
    Bu sonuÃ§, John Nash'in 1950 tarihli teoremine empirik doÄŸrulama: TaÅŸ-KaÄŸÄ±t-Makas gibi tam simetrik oyunlarda, Nash dengesi her aksiyonu eÅŸit olasÄ±lÄ±kla oynamaktÄ±r. Lineer programlama, minimax teoremini kullanarak bu dengeyi matematiksel kesinlikle hesaplÄ±yor. 5000 episode sonunda, ajanlarÄ±mÄ±z sadece bu teoremi "keÅŸfetmedi"â€Š-â€ŠLP sayesinde teorinin tam Ã§Ã¶zÃ¼mÃ¼nÃ¼ Ã¶ÄŸrendi.
    **Softmax vs LP KarÅŸÄ±laÅŸtÄ±rmasÄ±:** EÄŸer softmax kullanmÄ±ÅŸ olsaydÄ±k, Ã§ubuklar 0.33 civarÄ±nda olurdu ama tam 1/3'e oturmazdÄ±. Bu, yaklaÅŸÄ±k Ã§Ã¶zÃ¼m ile kesin Ã§Ã¶zÃ¼m arasÄ±ndaki farkÄ± gÃ¶steriyor.

### Ã‡Ä±ktÄ± Analizi
EÄŸitim tamamlandÄ±ktan sonra, sonuÃ§lar Nash dengesi teorisini matematiksel kesinlikle doÄŸrular:

*   **Nash PolitikalarÄ± (Lineer Programlama ile):** Her iki ajan da uniform mixed strategy'ye tam olarak yakÄ±nsadÄ±. Ajan 1 ve Ajan 2'nin politikalarÄ± [0.333â€¦, 0.333â€¦, 0.333â€¦] ideal deÄŸerini bulduâ€Š-â€Šbu, oyun teorisinin TaÅŸ-KaÄŸÄ±t-Makas iÃ§in matematiksel olarak kanÄ±tladÄ±ÄŸÄ± Nash dengesidir. Lineer programlama, softmax gibi yaklaÅŸÄ±k yÃ¶ntemlerin aksine, minimax teoreminin garantilediÄŸi kesin dengeyi hesaplar. HiÃ§bir ajan, rakibinin bu stratejisine karÅŸÄ± tek taraflÄ± deÄŸiÅŸiklik yaparak kazanÃ§ saÄŸlayamaz.
*   **Q-Matrisi YapÄ±sÄ±:** EÄŸitilmiÅŸ Q-tablolarÄ±, oyunun dÃ¶ngÃ¼sel doÄŸasÄ±nÄ± kristal berraklÄ±ÄŸÄ±nda yansÄ±tÄ±yor. Ajan 1'in matrisinde:
    *   Rock vs Scissors: +1.009 (rock, scissors'Ä± yener)
    *   Rock vs Paper: -0.994 (paper, rock'u yener)
    *   Paper vs Rock: +1.033 (paper, rock'u yener)
    *   Paper vs Scissors: -0.979 (scissors, paper'Ä± yener)
    *   Scissors vs Paper: +0.996 (scissors, paper'Ä± yener)
    *   Scissors vs Rock: -0.972 (rock, scissors'Ä± yener)
    
    Her kazanan-kaybeden Ã§ifti yaklaÅŸÄ±k Â±1.0 deÄŸerinde, bu da stokastik Ã¶dÃ¼l aralÄ±ÄŸÄ±mÄ±z (0.8â€“1.2) ile mÃ¼kemmel uyum iÃ§inde. Beraberlik durumlarÄ± (diagonal) 0.020 civarÄ±ndaâ€Š-â€Šneredeyse tam sÄ±fÄ±r. Bu, LP'nin Ã¼rettiÄŸi kesin Nash dengesinin bir gÃ¶stergesi.
*   **Simetri Analizi:** Ajan 2'nin Q-matrisi, Ajan 1 ile mÃ¼kemmel ayna simetrisi gÃ¶steriyor. Rock vs Paper iÃ§in +0.994 deÄŸeri, Paper vs Rock iÃ§in -0.994 ile tam karÅŸÄ±lÄ±k buluyor. Bu, her iki ajanÄ±n da aynÄ± optimal stratejiyi baÄŸÄ±msÄ±z olarak keÅŸfettiÄŸini kanÄ±tlar. Nash-Q Learning'in gÃ¼cÃ¼ tam da burada: Merkezi koordinasyon olmadan, sadece kendi deneyimlerinden Ã¶ÄŸrenerek denge noktasÄ±na ulaÅŸÄ±yorlar.
*   **Nash DeÄŸerleri:** Her iki ajanÄ±n gÃ¼venlik seviyesi (security level) sÄ±fÄ±ra Ã§ok yakÄ±n. Bu, simetrik bir oyunda beklenen sonuÃ§â€Š-â€Šuzun vadede hiÃ§bir ajan sistematik avantaj elde edemez.

---

## GerÃ§ek DÃ¼nya UygulamasÄ±: Siber GÃ¼venlikte Nash Dengesi
Nash-Q Learning'in teorik gÃ¼cÃ¼ etkileyici, peki gerÃ§ek dÃ¼nyada nasÄ±l kullanÄ±lÄ±yor? En heyecan verici uygulama alanlarÄ±ndan biri Ã§ok ajanlÄ± siber gÃ¼venlik simÃ¼lasyonlarÄ±.

### Senaryo: AÄŸ Penetrasyonu SimÃ¼lasyonu
Bir kurumsal aÄŸ dÃ¼ÅŸÃ¼nelim. SaldÄ±rgan ajanlar, aÄŸa sÄ±zmaya ve kritik verilere eriÅŸmeye Ã§alÄ±ÅŸÄ±yor. SavunmacÄ± ajanlar ise gÃ¼venlik duvarlarÄ±nÄ±, izleme sistemlerini ve yamalarÄ±nÄ± yÃ¶neterek saldÄ±rÄ±larÄ± engellemeye Ã§alÄ±ÅŸÄ±yor. Bu, klasik bir sÄ±fÄ±r toplamlÄ± olmayan stokastik oyundur:
*   **Durumlar:** AÄŸÄ±n mevcut gÃ¼venlik konfigÃ¼rasyonu, aktif baÄŸlantÄ±lar, tespit edilen anormallikler.
*   **Aksiyonlar:** SaldÄ±rganlar iÃ§in: port tarama, exploit deneme, lateral hareket. SavunmacÄ±lar iÃ§in: port kapatma, trafik filtreleme, sistem gÃ¼ncellemeleri.
*   **Ã–dÃ¼ller:** SaldÄ±rganlar kritik verilere eriÅŸtiklerinde pozitif, yakalandÄ±klarÄ±nda negatif Ã¶dÃ¼l alÄ±r. SavunmacÄ±lar tam tersi.
*   **Stokastiklik:** Exploit'lerin baÅŸarÄ± olasÄ±lÄ±ÄŸÄ±, aÄŸ gecikmesi, tespit sistemlerinin hassasiyeti rastgele faktÃ¶rlerdir.

2025 yÄ±lÄ±nda yayÄ±nlanan *"Nash Q-Network for Multi-Agent Cybersecurity Simulation"* makalesi, bu senaryoda Nash-Q Learning'in derin sinir aÄŸlarÄ± ile birleÅŸtirilmesini araÅŸtÄ±rdÄ±. AraÅŸtÄ±rmacÄ±lar, MITRE ATT&CK framework'Ã¼ndeki gerÃ§ek saldÄ±rÄ± verilerini kullanarak simÃ¼lasyonlar yaptÄ±. SonuÃ§lar etkileyici:
*   **Daha iyi denge:** Nash-Q tabanlÄ± savunmacÄ±lar, geleneksel rule-based sistemlere gÃ¶re daha robust politikalar Ã¶ÄŸrendi. SaldÄ±rganlar stratejilerini deÄŸiÅŸtirdiÄŸinde, Nash-Q hÄ±zla adapte oldu.
*   **Konverjans hÄ±zÄ±:** Derin Nash-Q Network kullanÄ±mÄ±, konverjans sÃ¼resini bÃ¼yÃ¼k Ã¶lÃ§Ã¼de azalttÄ±. Bu, gerÃ§ek zamanlÄ± tehdit Ã¶nleme iÃ§in kritik.
*   **Genelleme:** EÄŸitilmiÅŸ model, eÄŸitim setinde olmayan yeni saldÄ±rÄ± senaryolarÄ±nda da baÅŸarÄ±lÄ± savunma stratejileri Ã¼retti.

BaÅŸka bir ilgili Ã§alÄ±ÅŸma, *"A multi-step minimax Q-learning"* yaklaÅŸÄ±mÄ±nÄ± zero-sum senaryolarda pratik hesaplama iÃ§in kullandÄ±. Bu yÃ¶ntem, Nash dengesini hesaplamak iÃ§in gereken karmaÅŸÄ±k lineer programlama yerine, iteratif minimax aramasÄ±yla yaklaÅŸÄ±k Ã§Ã¶zÃ¼mler buldu. BÃ¼yÃ¼k Ã¶lÃ§ekli aÄŸlarda, bu yaklaÅŸÄ±m hesaplama sÃ¼resini saatlerden dakikalara indirdi.

### DiÄŸer Potansiyel Uygulamalar
*   **Otonom AraÃ§ TrafiÄŸi:** Ã‡ok sayÄ±da otonom aracÄ±n aynÄ± kavÅŸakta buluÅŸtuÄŸu senaryolarda, her araÃ§ kendi rotasÄ±nÄ± optimize ederken diÄŸer araÃ§larÄ±n hareketlerini de dikkate almalÄ±dÄ±r. Nash-Q Learning, Ã§arpÄ±ÅŸma Ã¶nleme ve trafik akÄ±ÅŸÄ± optimizasyonu iÃ§in dengeli politikalar Ã¶ÄŸrenebilir.
*   **Enerji AÄŸÄ± Kaynak DaÄŸÄ±tÄ±mÄ±:** AkÄ±llÄ± ÅŸebekelerde, birden fazla Ã¼retici ve tÃ¼ketici ajanÄ±n etkileÅŸimi Nash dengesiyle modellenebilir. Her ajan enerjiyi ne zaman Ã¼retip tÃ¼kettiÄŸini optimize ederken, Nash-Q Learning sistem genelinde dengeli bir daÄŸÄ±tÄ±m saÄŸlar.
*   **Drone SÃ¼rÃ¼leri:** Askeri veya lojistik uygulamalarda, drone'larÄ±n koordineli hareket etmesi gerekir. Nash-Q, her drone'un baÄŸÄ±msÄ±z karar verirken sÃ¼rÃ¼ hedeflerine katkÄ±da bulunmasÄ±nÄ± saÄŸlayabilir.

---

## AÃ§Ä±k Sorular ve SonuÃ§: DÃ¼ÅŸÃ¼nmeye Davet Ediyoruz
Nash-Q Learning, Ã§ok ajanlÄ± pekiÅŸtirmeli Ã¶ÄŸrenmede Ã¶nemli bir adÄ±m olsa da, hala cevaplanmayÄ± bekleyen birÃ§ok soru var:
*   **KÄ±smi GÃ¶zlemlenebilir Ortamlar:** Nash-Q, tam gÃ¶zlemlenebilir ortamlar iÃ§in tasarlandÄ±. Peki, POMDP'lerde (Partially Observable Markov Decision Processes) nasÄ±l Ã¶lÃ§eklenebilir? Gelecek haftaki yazÄ±mÄ±zda gÃ¶receÄŸimiz regret minimization teknikleri mi yoksa derin Ã¶ÄŸrenme ile belief-state tracking mi daha etkili?
*   **Konverjans Gecikmesi:** Stokastik oyunlarda Nash dengesine yakÄ±nsama, Ã¶zellikle bÃ¼yÃ¼k durum-aksiyon uzaylarÄ±nda Ã§ok yavaÅŸ olabilir. GerÃ§ek zamanlÄ± uygulamalardaâ€Š-â€ŠÃ¶rneÄŸin drone swarm'larÄ± veya yÃ¼ksek frekanslÄ± ticaret sistemlerindeâ€Š-â€Šbu gecikme nasÄ±l aÅŸÄ±labilir? YaklaÅŸÄ±k Nash dengesi yeterli mi?
*   **Ä°ÅŸbirlikÃ§i-RekabetÃ§i Hibrit Oyunlar:** Von Neumann'Ä±n minimax'Ä± ve Nash dengesi, tamamen rekabetÃ§i veya tamamen baÄŸÄ±msÄ±z ajanlar iÃ§in tasarlandÄ±. Ancak iklim deÄŸiÅŸikliÄŸi mÃ¼zakereleri, uluslararasÄ± ticaret veya ortak kaynak yÃ¶netimi gibi hibrit senaryolarda durum nasÄ±l? Nash-Q, iÅŸbirliÄŸi ve rekabeti aynÄ± anda modelleyebilir mi?
*   **Derin Nash-Q'nun SÄ±nÄ±rlarÄ±:** Derin Ã¶ÄŸrenme ile Nash-Q'nun birleÅŸimi umut verici, ancak derin aÄŸlarÄ±n eÄŸitim kararsÄ±zlÄ±ÄŸÄ± ve overfitting riski var. Multi-agent ortamlarda bu sorunlar nasÄ±l minimize edilir?

### Ã–zet ve Sonraki AdÄ±m
Bu yazÄ±da, Nash-Q Learning'in oyun teorisi ve pekiÅŸtirmeli Ã¶ÄŸrenme arasÄ±ndaki kÃ¶prÃ¼yÃ¼ nasÄ±l kurduÄŸunu gÃ¶rdÃ¼k. Von Neumann'Ä±n minimax felsefesinden Nash'in denge kavramÄ±na, oradan Q-learning'in adaptif gÃ¼cÃ¼ne uzanan bu yolculuk, MARL'Ä± denge odaklÄ± hale getirdi. Stokastik TaÅŸ-KaÄŸÄ±t-Makas Ã¶rneÄŸimizle teoriyi pratiÄŸe dÃ¶ktÃ¼k, siber gÃ¼venlik simÃ¼lasyonlarÄ±yla gerÃ§ek dÃ¼nya etkisini gÃ¶rdÃ¼k.

Ancak hikaye burada bitmiyor. Nash dengesi, tam bilgi oyunlarÄ±nda gÃ¼Ã§lÃ¼ bir araÃ§, peki ya imperfect-information oyunlarda? Poker gibi oyunlarda, rakibinizin elini bilmediÄŸiniz ve bluff'un hayati olduÄŸu durumlarda denge nasÄ±l bulunur?

BeÄŸendiyseniz ğŸ‘ alkÄ±ÅŸlayÄ±n ve takip edin! Ã–nÃ¼mÃ¼zdeki 49 hafta boyunca bu yolculukta birlikte olalÄ±m. SorularÄ±nÄ±z varsa yorumlarda buluÅŸalÄ±m.

**Bir sonraki yazÄ±da gÃ¶rÃ¼ÅŸmek Ã¼zere! Gelecek hafta: "Ã‡ok Oyunculu Poker'de Denge ArayÄ±ÅŸÄ±: Counterfactual Regret Minimization'Ä±n SÄ±rlarÄ±"**

**GitHub'da Kod:** [github.com/highcansavci/marl-game-theory](https://github.com/highcansavci/marl-game-theory) â†’ TÃ¼m kod Ã¶rnekleri, notebook'lar ve ekstra materyaller

**Ä°letiÅŸim:** [highcsavci@gmail.com]â€Š-â€ŠSorularÄ±nÄ±z, Ã¶nerileriniz her zaman hoÅŸ gelir
