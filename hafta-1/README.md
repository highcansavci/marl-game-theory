# Neden Tek Ajan Yetmiyor? MARL ve Oyun Teorisinin DoÄŸuÅŸu

Gelin size bir hikaye anlatayÄ±m.

Bir gÃ¼n AlphaZero'ya diyorsunuz ki: "Hadi satranÃ§ oynayalÄ±m." AlphaZero sÃ¼per mutlu, Ã§Ã¼nkÃ¼ milyonlarca oyun oynamÄ±ÅŸ, dÃ¼nyanÄ±n en iyi stratejilerini Ã¶ÄŸrenmiÅŸ. Ama bir anda kurallarÄ± deÄŸiÅŸtiriyorsunuz - her hamleden sonra tahtayÄ± biraz karÄ±ÅŸtÄ±rÄ±yorsunuz. TaÅŸlarÄ± random yerlere koyuyorsunuz. AlphaZero ne yapabilir? HiÃ§bir ÅŸey. Ã‡Ã¼nkÃ¼ Ã¶ÄŸrendiÄŸi her ÅŸey "sabit bir dÃ¼nya" varsayÄ±mÄ±na dayanÄ±yordu.

Åimdi daha da ilginÃ§ bir senaryo: TahtayÄ± karÄ±ÅŸtÄ±rmÄ±yorsunuz ama rakibiniz sÃ¼rekli yeni taktikler deniyor. Siz sol kanattan saldÄ±rÄ±nca merkezi kapatÄ±yor, merkeze yÃ¼klenince kanat aÃ§Ä±yor. Bu rakip Ã¶ÄŸreniyor, adapte oluyor, strateji geliÅŸtiriyor. Ä°ÅŸte tam bu noktada klasik yapay zeka patlamaya baÅŸlÄ±yor. Ã‡Ã¼nkÃ¼ artÄ±k sabit bir "ortam" yok - **karÅŸÄ±nÄ±zda stratejik dÃ¼ÅŸÃ¼nen baÅŸka bir zihin var.**

Ve iÅŸte bu yazÄ±da tam olarak bunu konuÅŸacaÄŸÄ±z: Neden tek baÅŸÄ±na hareket eden ajanlar gerÃ§ek dÃ¼nyada yetersiz kalÄ±yor ve Ã§oklu ajan sistemleri + oyun teorisi birleÅŸimi nasÄ±l devrim yaratÄ±yor.

---

## Tek Ajan Reinforcement Learning: Parlak BaÅŸarÄ±lar, Gizli SÄ±nÄ±rlar

Ã–nce biraz gerilere gidelim.

Son 10 yÄ±lda pekiÅŸtirmeli Ã¶ÄŸrenme (Reinforcement Learning) mucizevi ÅŸeyler baÅŸardÄ±. DQN Atari oyunlarÄ±nÄ± Ã§Ã¶zdÃ¼, AlphaGo dÃ¼nya ÅŸampiyonunu yendi, robotlar hassas manipÃ¼lasyon Ã¶ÄŸrendi. MuhteÅŸem deÄŸil mi?

Ama hepsinin ortak bir noktasÄ± var: **Ortam sizden baÄŸÄ±msÄ±z davranÄ±yor.**

DÃ¼ÅŸÃ¼nÃ¼n: Tetris oynuyorsunuz. Bloklar yukarÄ±dan dÃ¼ÅŸÃ¼yor. Siz ne kadar iyi veya kÃ¶tÃ¼ oynarsanÄ±z oynayÄ±n, bloklar gelmeye devam ediyor - sizin stratejinize gÃ¶re davranÄ±ÅŸlarÄ±nÄ± deÄŸiÅŸtirmiyorlar. Super Mario'daki Goomba'lar hep aynÄ± ÅŸekilde yÃ¼rÃ¼yor. Pong'daki top fizik kurallarÄ±na uyuyor.

Matematik dilinde buna **Markov Decision Process (MDP)** diyoruz.

Bu terimi gÃ¶rÃ¼nce korkmayÄ±n! Sadece ÅŸunu sÃ¶ylÃ¼yor: Bir durum uzayÄ±nÄ±z var, eylemleriniz var, bu eylemlerin sonuÃ§larÄ± var (geÃ§iÅŸ olasÄ±lÄ±klarÄ±), ve aldÄ±ÄŸÄ±nÄ±z Ã¶dÃ¼ller var. Ve hepsi **deterministik** - yani sabit kurallarla Ã§alÄ±ÅŸÄ±yor.

Peki ya ÅŸimdi karÅŸÄ±nÄ±za bilinÃ§li bir rakip Ã§Ä±ksa? Sizin her hamlenize gÃ¶re yeni bir strateji geliÅŸtirse? Ä°ÅŸte burada iÅŸler karÄ±ÅŸÄ±yor.

### Non-Stationarity: SÃ¼rekli DeÄŸiÅŸen Oyun AlanÄ±

Åimdi dÃ¼ÅŸÃ¼nÃ¼n: Ä°ki robot aynÄ± depoda Ã§alÄ±ÅŸÄ±yor. Ä°kisi de "en hÄ±zlÄ± ÅŸekilde paket topla" gÃ¶revini Ã¶ÄŸreniyor. Robot A bir koridor seÃ§iyor, hÄ±zlÄ±ca gidip geliyor. SÃ¼per! Ama Robot B de aynÄ± koridoru seÃ§erse ne olur? Ã‡arpÄ±ÅŸma. YavaÅŸlama. Verimsizlik.

Robot A iÃ§in ortam artÄ±k sabit deÄŸil. Ã‡Ã¼nkÃ¼ Robot B'nin davranÄ±ÅŸÄ± sÃ¼rekli deÄŸiÅŸiyor - o da Ã¶ÄŸreniyor!

Robot A'nÄ±n geÃ§iÅŸ fonksiyonu artÄ±k kendi eylemlerine deÄŸil, Robot B'nin eylemlerine de baÄŸlÄ±. Ve Robot B sÃ¼rekli deÄŸiÅŸiyor!

Ä°ÅŸte buna **non-stationarity** diyoruz. OrtamÄ±nÄ±z durgun deÄŸil, akan bir nehir gibi sÃ¼rekli evrim geÃ§iriyor. Klasik RL algoritmalarÄ± bu durumda sarsÄ±lÄ±yor Ã§Ã¼nkÃ¼ temel varsayÄ±mlarÄ± Ã§Ã¶kÃ¼yor.

---

## Oyun Teorisi Devreye Giriyor: John Nash'in Dahice Fikri

1950 yÄ±lÄ±nda genÃ§ bir matematikÃ§i olan John Nash (evet, "Beautiful Mind" filmindeki adam) ÅŸÃ¶yle bir soru soruyor:

> "EÄŸer herkes akÄ±llÄ±ysa ve herkes birbirinin akÄ±llÄ± olduÄŸunu biliyorsa, nasÄ±l bir denge noktasÄ±na ulaÅŸÄ±rÄ±z?"

Ve cevabÄ± o kadar zarif ki: **Nash Dengesi**.

Siz hiÃ§ dÃ¼ÅŸÃ¼ndÃ¼nÃ¼z mÃ¼, trafik Ä±ÅŸÄ±klarÄ±nda herkes kÄ±rmÄ±zÄ±da duruyor ama aslÄ±nda zorunlu deÄŸil. GeÃ§ebilirsiniz. Ama geÃ§miyorsunuz. Neden? Ã‡Ã¼nkÃ¼ herkes ÅŸunu biliyor: "EÄŸer herkes geÃ§erse kaza olur, herkes kaybeder. EÄŸer herkes durursa gÃ¼venli geÃ§iÅŸ olur, herkes kazanÄ±r." Ve bir denge noktasÄ± oluÅŸuyor.

Ä°ÅŸte Nash dengesi tam olarak bu: HiÃ§ kimsenin tek baÅŸÄ±na stratejisini deÄŸiÅŸtirerek daha iyi sonuÃ§ alamayacaÄŸÄ± nokta.

Formel olarak (kÄ±sa tutuyorum): Bu, bir ajanÄ±n kendi stratejisini deÄŸiÅŸtirdiÄŸinde, diÄŸerleri sabit kalÄ±rsa, daha kÃ¶tÃ¼ bir sonuÃ§ alacaÄŸÄ± anlamÄ±na gelir.

### Best Response: KarÅŸÄ±nÄ±zdakine GÃ¶re En Ä°yi Hamle

Åimdi gÃ¼zel bir kavram daha: **Best Response** (En Ä°yi Tepki).

Futbol maÃ§Ä±nda penaltÄ± atÄ±yorsunuz. Kaleci sol tarafa atlarsa saÄŸa vurmak en iyi tepkiniz. Kaleci saÄŸa atlarsa sol. Kaleci ortada kalÄ±rsa orta. Sizin en iyi tepkiniz, kalecinin stratejisine baÄŸlÄ±.

Matematiksel olarak bu, "DiÄŸerlerinin stratejisi buysa, benim en iyi stratejim budur" demektir.

Nash dengesi ne zaman oluÅŸur? Herkes aynÄ± anda birbirinin en iyi tepkisini oynadÄ±ÄŸÄ±nda! Bu yÃ¼zden denge.

---

## Markov OyunlarÄ±: MDP'nin Ã‡oklu Ajan Versiyonu

Tamam, ÅŸimdi hepsini bir araya getirelim. Tek ajan MDP'miz vardÄ±. Åimdi onu Ã§oklu ajana geniÅŸletiyoruz ve ortaya **Markov Oyunu** (Stochastic Game) Ã§Ä±kÄ±yor.

Burada yeni olanlar: Ajan sayÄ±sÄ±, her ajanÄ±n kendi eylem uzayÄ± ve en Ã¶nemlisi, her ajanÄ±n **kendi Ã¶dÃ¼l fonksiyonu** var!

Bu son nokta Ã§ok kritik. Ã‡Ã¼nkÃ¼ ÅŸimdi Ã¼Ã§ farklÄ± oyun tipi ortaya Ã§Ä±kÄ±yor:

**1. Tam Ä°ÅŸbirlikÃ§i Oyunlar:** Herkesin Ã¶dÃ¼lÃ¼ aynÄ±. "Hepimiz aynÄ± gemideyiz."
   - Ã–rnek: Robotlar beraber bir nesne taÅŸÄ±yor

**2. Zero-Sum Oyunlar:** Birinin kazancÄ± diÄŸerinin kaybÄ±.
   - Ã–rnek: SatranÃ§, poker, futbol maÃ§Ä±

**3. General-Sum Oyunlar:** Herkesin farklÄ± hedefi var, bazen Ã§akÄ±ÅŸÄ±yor bazen uyuÅŸuyor.
   - Ã–rnek: Trafikteki araÃ§lar (hÄ±z istiyorsunuz ama kazasÄ±z)

Bellman denklemi de deÄŸiÅŸiyor tabii. ArtÄ±k tek bir politika yok, ortak politika var. ArtÄ±k tÃ¼m ajanlarÄ±n ortak eylemi Ã¼zerinden hesaplÄ±yoruz. Ã‡Ã¼nkÃ¼ ortam herkese baÄŸlÄ±.

---

## Kod ZamanÄ±: Basit Ama Derin Bir Ã–rnek

Teoriyi yaptÄ±k, ÅŸimdi pratiÄŸe geÃ§elim. Size Ã§ok basit ama Ã§ok Ã¶ÄŸretici bir Ã¶rnek gÃ¶stereceÄŸim: **BuluÅŸma NoktasÄ± Oyunu**.

Ä°ki arkadaÅŸ ÅŸehirde buluÅŸmaya Ã§alÄ±ÅŸÄ±yor. Ä°ki seÃ§enek var: Park veya Kafe. Ä°kisi de aynÄ± yere giderse buluÅŸurlar (+10 puan). FarklÄ± yerlere giderse buluÅŸamazlar (-5 puan).

Bu oyunun iki Nash dengesi var: Her ikisi de Park'a gider, veya her ikisi de Kafe'ye gider. Ama hangisini seÃ§ecekler? Ä°ÅŸte asÄ±l soru bu!

```python
import torch
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt

class MeetingGame:
    """Ä°ki arkadaÅŸ buluÅŸmaya Ã§alÄ±ÅŸÄ±yor: Park (0) veya Kafe (1)"""
    def __init__(self):
        # Ã–dÃ¼l matrisi: [ajan1_seÃ§imi, ajan2_seÃ§imi]
        self.rewards = {
            (0, 0): (10, 10),   # Ä°kisi de Park -> BuluÅŸma!
            (0, 1): (-5, -5),   # Biri Park, biri Kafe -> BuluÅŸamama
            (1, 0): (-5, -5),   # Biri Kafe, biri Park -> BuluÅŸamama  
            (1, 1): (10, 10),   # Ä°kisi de Kafe -> BuluÅŸma!
        }
    
    def step(self, actions):
        """Ajanlar karar veriyor, Ã¶dÃ¼ller hesaplanÄ±yor"""
        a1, a2 = actions
        r1, r2 = self.rewards[(a1, a2)]
        return r1, r2

class IndependentQLearner:
    """Her ajan baÄŸÄ±msÄ±z Q-learning yapÄ±yor (diÄŸerinden habersiz)"""
    def __init__(self, n_actions=2, lr=0.1, gamma=0.95, epsilon=0.1):
        self.n_actions = n_actions
        self.lr = lr
        self.gamma = gamma
        self.epsilon = epsilon
        self.Q = np.zeros(n_actions)  # Q-deÄŸerleri
    
    def choose_action(self):
        """Epsilon-greedy: Bazen keÅŸfet, bazen en iyiyi seÃ§"""
        if np.random.random() < self.epsilon:
            return np.random.randint(self.n_actions)  # KeÅŸif
        return np.argmax(self.Q)  # En iyi
    
    def learn(self, action, reward):
        """Q-learning gÃ¼ncellemesi"""
        # Bellman update: Q(a) <- Q(a) + lr * (r + gamma*max(Q) - Q(a))
        target = reward + self.gamma * np.max(self.Q)
        self.Q[action] += self.lr * (target - self.Q[action])

# Oyunu baÅŸlatalÄ±m!
game = MeetingGame()
agent1 = IndependentQLearner(epsilon=0.15)  # Ä°lk ajan
agent2 = IndependentQLearner(epsilon=0.15)  # Ä°kinci ajan

# Ã–ÄŸrenme sÃ¼reci
n_episodes = 5000
history = {'rewards': [], 'actions1': [], 'actions2': []}

print("Ã–ÄŸrenme baÅŸlÄ±yor... Her ajan baÄŸÄ±msÄ±z karar veriyor.\n")

for episode in range(n_episodes):
    # Ä°ki ajan da kendi Q-tablolarÄ±na bakarak karar veriyor
    a1 = agent1.choose_action()
    a2 = agent2.choose_action()
    
    # Oyun oynuyoruz ve sonuÃ§larÄ± gÃ¶rÃ¼yoruz
    r1, r2 = game.step((a1, a2))
    
    # Her ajan kendi deneyiminden Ã¶ÄŸreniyor (diÄŸerinden habersiz!)
    agent1.learn(a1, r1)
    agent2.learn(a2, r2)
    
    # Ä°statistik tutuyoruz
    history['rewards'].append((r1 + r2) / 2)
    history['actions1'].append(a1)
    history['actions2'].append(a2)

# SonuÃ§larÄ± gÃ¶relim
print("=== Ã–ÄŸrenme TamamlandÄ± ===\n")
print(f"Ajan 1 Q-deÄŸerleri: Park={agent1.Q[0]:.2f}, Kafe={agent1.Q[1]:.2f}")
print(f"Ajan 2 Q-deÄŸerleri: Park={agent2.Q[0]:.2f}, Kafe={agent2.Q[1]:.2f}")

# Test: Son 1000 bÃ¶lÃ¼mde ne kadar koordine olabildiler?
last_actions = list(zip(history['actions1'][-1000:], history['actions2'][-1000:]))
coordination_rate = sum(1 for a1, a2 in last_actions if a1 == a2) / 1000
print(f"\nKoordinasyon BaÅŸarÄ±sÄ±: %{coordination_rate*100:.1f}")
print(f"(Ne kadar sÄ±klÄ±kla aynÄ± yeri seÃ§tiler?)")

# Hangi dengeye yakÄ±nsadÄ±lar?
park_rate = sum(1 for a1, a2 in last_actions if a1 == 0 and a2 == 0) / 1000
cafe_rate = sum(1 for a1, a2 in last_actions if a1 == 1 and a2 == 1) / 1000
print(f"\nNash Dengesi SeÃ§imi:")
print(f"  Park-Park: %{park_rate*100:.1f}")
print(f"  Kafe-Kafe: %{cafe_rate*100:.1f}")

# GÃ¶rselleÅŸtirme
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Sol: Ortalama Ã¶dÃ¼l zamanla
window = 100
smoothed = np.convolve(history['rewards'], np.ones(window)/window, mode='valid')
ax1.plot(smoothed, color='#2E86AB', linewidth=2)
ax1.axhline(y=10, color='green', linestyle='--', alpha=0.5, label='Optimal (Koordinasyon)')
ax1.axhline(y=-5, color='red', linestyle='--', alpha=0.5, label='En KÃ¶tÃ¼ (KarÄ±ÅŸÄ±klÄ±k)')
ax1.fill_between(range(len(smoothed)), -5, smoothed, alpha=0.2, color='#2E86AB')
ax1.set_xlabel('BÃ¶lÃ¼m (Episode)', fontsize=12)
ax1.set_ylabel('Ortalama Ã–dÃ¼l', fontsize=12)
ax1.set_title('Zaman Ä°Ã§inde Koordinasyon Ã–ÄŸrenimi', fontsize=14, fontweight='bold')
ax1.legend()
ax1.grid(alpha=0.3)

# SaÄŸ: Son durumda Q-deÄŸerleri
locations = ['Park', 'Kafe']
x = np.arange(len(locations))
width = 0.35
ax2.bar(x - width/2, agent1.Q, width, label='Ajan 1', color='#A23B72', alpha=0.8)
ax2.bar(x + width/2, agent2.Q, width, label='Ajan 2', color='#F18F01', alpha=0.8)
ax2.set_ylabel('Q-DeÄŸeri (Beklenen Ã–dÃ¼l)', fontsize=12)
ax2.set_title('Her AjanÄ±n Tercih DeÄŸerleri', fontsize=14, fontweight='bold')
ax2.set_xticks(x)
ax2.set_xticklabels(locations)
ax2.legend()
ax2.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('coordination_learning.png', dpi=150, bbox_inches='tight')
print("\nâœ“ Grafik kaydedildi: coordination_learning.png")
```
[![SonuÃ§lar](coordination_learning.png)](coordination_learning.png)

### Bu Kodda Ne Oluyor?

Dikkat edin, her ajan **tamamen baÄŸÄ±msÄ±z** Ã¶ÄŸreniyor. Ajan 1, Ajan 2'nin Q-tablosunu gÃ¶rmÃ¼yor. Sadece kendi deneyiminden Ã¶ÄŸreniyor. Bu **independent learning** yaklaÅŸÄ±mÄ±.

Ve ÅŸaÅŸÄ±rtÄ±cÄ± bir ÅŸey oluyor: Ajanlar zamanla koordine olmayÄ± Ã¶ÄŸreniyorlar! Ama hangi dengeye gidecekler (Park-Park mÄ±, Kafe-Kafe mi) tamamen **rastgele baÅŸlangÄ±Ã§ koÅŸullarÄ±na ve erken deneyimlere** baÄŸlÄ±.

Bazen ikisi de Park'Ä± seviyor, bazen Kafe'yi. Ama sonunda birbirlerini anlÄ±yorlar.

**Kritik Nokta:** Bu kod, oyun teorisinin en derin sorusunu gÃ¶steriyor: **Equilibrium selection problem**. Birden fazla Nash dengesi varsa, hangisine gideriz? Cevap: Tarih baÄŸÄ±mlÄ± (path-dependent). Ä°lk deneyimler geleceÄŸi ÅŸekillendiriyor.

---

## GerÃ§ek DÃ¼nyada MARL + Oyun Teorisi: ÃœÃ§ BÃ¼yÃ¼leyici Ã–rnek

### 1. Otonom AraÃ§lar: Sessiz MÃ¼zakere

San Francisco'da Waymo araÃ§larÄ± artÄ±k yolda. Bir kavÅŸaÄŸa geliyorlar. DÃ¶rt araÃ§ var, hepsi otonom. HiÃ§ konuÅŸmadan, hiÃ§ sinyal vermeden, anlÄ±k olarak "kim Ã¶nce geÃ§ecek?" sorusunu Ã§Ã¶zmeleri gerekiyor.

Ã‡Ã¶zÃ¼m? Oyun teorik MARL. Her araÃ§ bir ajan, her ajan diÄŸerlerinin hÄ±zÄ±nÄ±, konumunu, amacÄ±nÄ± modelliyor. Ve bir **Nash dengesi** arÄ±yorlar: HiÃ§bir aracÄ±n tek baÅŸÄ±na kuralÄ± Ã§iÄŸneyerek daha iyi sonuÃ§ alamayacaÄŸÄ± gÃ¼venli geÃ§iÅŸ stratejisi.

SonuÃ§: %40 daha hÄ±zlÄ± trafik akÄ±ÅŸÄ±, sÄ±fÄ±r kaza. Ã‡Ã¼nkÃ¼ herkes birbirinin ne dÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼nÃ¼ dÃ¼ÅŸÃ¼nÃ¼yor.

### 2. AlphaStar: StarCraft II'de Stratejik Derinlik

Blizzard'Ä±n StarCraft II oyunu, belki de dÃ¼nyanÄ±n en karmaÅŸÄ±k strateji oyunu. AlphaStar bunu Ã§Ã¶zmek iÃ§in ne yaptÄ±?

**League Training:** FarklÄ± stratejilerdeki ajanlardan oluÅŸan bir "lig" oluÅŸturdular. Her ajan hem Main agents'larla (mevcut en iyiler), hem Exploiter agents'larla (zayÄ±f noktalarÄ± bulan), hem Main Exploiter agents'larla (en iyi ajanlarÄ±n zayÄ±f noktalarÄ±nÄ± bulan) oynadÄ±.

Bu ne? **Ã‡ok oyunculu oyun teorik denge arayÄ±ÅŸÄ±!** Tek bir Nash dengesi yok, sÃ¼rekli evrimleÅŸen bir **meta-game** var. Ve AlphaStar bunu Ã¶ÄŸrendi.

SonuÃ§: Profesyonel oyuncularÄ± yendi. Ama daha Ã¶nemlisi, hiÃ§ gÃ¶rÃ¼lmemiÅŸ stratejiler geliÅŸtirdi - tÄ±pkÄ± insanlarÄ±n yapacaÄŸÄ± gibi.

### 3. Finansal Piyasalar: AlgoritmalarÄ±n Dans EttiÄŸi Yer

New York BorsasÄ±'nda saniyede binlerce iÅŸlem yapan yÃ¼zlerce trading botu var. Her biri diÄŸerlerinin davranÄ±ÅŸÄ±nÄ± tahmin etmeye Ã§alÄ±ÅŸÄ±yor.

EÄŸer Nash dengesi olmasaydÄ±? Kaos. Volatilite. Flash crash'ler (2010'da yaÅŸandÄ±).

Ama oyun teorik MARL algoritmalarÄ± sayesinde, botlar bir tÃ¼r "sessiz anlaÅŸma"ya varÄ±yorlar. AÅŸÄ±rÄ± rekabetÃ§i davranmÄ±yorlar Ã§Ã¼nkÃ¼ herkesin kaybetmesine yol aÃ§Ä±yor. Market maker botlar spread'i dengede tutuyorlar.

Ve ilginÃ§tir ki bu **kimsenin program etmediÄŸi** bir davranÄ±ÅŸ - dengelerden ortaya Ã§Ä±kÄ±yor (emergent behavior).

---

## Sizi DÃ¼ÅŸÃ¼nmeye Davet Ediyorum

Geldik en sevdiÄŸim kÄ±sma. Åimdi size bazÄ± sorular soracaÄŸÄ±m. CevaplarÄ± bilmiyorum. Kimse bilmiyor. Ama bunlar Ã¼zerine dÃ¼ÅŸÃ¼nmek, MARL'Ä±n derinliklerini anlamanÄ±n anahtarÄ±.

**Soru 1: Non-Stationarity Paradoksu**

EÄŸer tÃ¼m ajanlar sÃ¼rekli Ã¶ÄŸreniyorsa ve deÄŸiÅŸiyorsa, "optimal politika" diye bir ÅŸey hala var mÄ±? Yoksa sÃ¼rekli kovaladÄ±ÄŸÄ±mÄ±z ama asla yakalayamadÄ±ÄŸÄ±mÄ±z bir hedef mi?

**Soru 2: Koordinasyon MÄ±, Rekabet Mi?**

Ä°ki Nash dengesi var: (Ä°ÅŸbirliÄŸi yap, %50 kazan) veya (Rekabet et, %30 kazan ama daha hÄ±zlÄ±). Ajanlar hangisini seÃ§er? Ve bu seÃ§im topluma nasÄ±l yayÄ±lÄ±r?

**Soru 3: Ä°nsan FaktÃ¶rÃ¼**

Ä°nsanlar her zaman rasyonel deÄŸil. Bazen duygusal davranÄ±yoruz, bazen yanÄ±lÄ±yoruz, bazen sadece deneme yapÄ±yoruz. MARL ajanlarÄ± bu "bounded rationality"yi nasÄ±l modellemeli?

**Soru 4: Ã–lÃ§ek Problemi**

100 ajan olduÄŸunda Nash dengesi hesaplamak pratik mi? Ya 10,000 ajan? YaklaÅŸÄ±k Ã§Ã¶zÃ¼mler yeterli mi yoksa kesin dengeler mi gerekli?

**Soru 5: Emergent Behavior**

Basit Ã¶ÄŸrenme kurallarÄ±yla baÅŸlayan ajanlar, karmaÅŸÄ±k sosyal normlarÄ± (adalet, gÃ¼ven, reciprocity) kendiliÄŸinden geliÅŸtirebilir mi? Bu normlarÄ±n "doÄŸal" mÄ± yoksa "Ã¶ÄŸrenilmiÅŸ" mi olduÄŸu fark eder mi?

---

## Gelecek Hafta: Self-Play'in Derin SÄ±rlarÄ±

Bir sonraki yazÄ±mÄ±zda, muhtemelen MARL'Ä±n en gÃ¼Ã§lÃ¼ konseptine dalacaÄŸÄ±z: **Self-Play**.

AlphaGo kendisiyle oynayarak Go'yu Ã¶ÄŸrendi. AlphaStar kendisiyle oynayarak StarCraft'Ä± Ã§Ã¶zdÃ¼. OpenAI Five Dota 2'de dÃ¼nya ÅŸampiyonlarÄ±nÄ± yendi - yine self-play ile.

Ama self-play'in altÄ±nda yatan oyun teorik prensipleri neler? Fictitious play nedir? Population-based training nasÄ±l Ã§alÄ±ÅŸÄ±r? Ve en Ã¶nemlisi: NasÄ±l oluyor da ajanlar kendileriyle oynayarak, hiÃ§ gÃ¶rmeyecekleri rakipleri yenebiliyorlar?

**Spoiler:** Cevap, evrimsel oyun teorisinde gizli. Ve inanÄ±lmaz gÃ¼zel.

---

## Kaynaklar: Daha Derine Ä°nmek Ä°steyenler Ä°Ã§in

### BaÅŸlangÄ±Ã§ Seviyesi
- **Sutton & Barto (2018)** - "Reinforcement Learning: An Introduction" â†’ RL'nin Ä°ncili, Ã¼cretsiz online
- **Leyton-Brown & Shoham (2008)** - "Essentials of Game Theory" â†’ Oyun teorisine yumuÅŸak giriÅŸ

### Orta Seviye
- **Littman (1994)** - "Markov Games as a Framework for Multi-Agent RL" â†’ Markov oyunlarÄ±nÄ±n kurucusu
- **Busoniu et al. (2008)** - "A Comprehensive Survey of MARL" â†’ GeniÅŸ literatÃ¼r taramasÄ±

### Ä°leri Seviye
- **Zhang et al. (2021)** - "Multi-Agent RL: A Selective Overview of Theories and Algorithms" â†’ Modern perspektif
- **Lanctot et al. (2017)** - "A Unified Game-Theoretic Approach to MARL" â†’ DeepMind'Ä±n kapsamlÄ± Ã§alÄ±ÅŸmasÄ±

### Pratik Uygulamalar
- **OpenAI Gym** - Ã‡oklu ajan ortamlarÄ± (PettingZoo kÃ¼tÃ¼phanesi)
- **RLlib (Ray)** - Production-ready MARL framework
- **DeepMind Open Source** - AlphaStar, AlphaGo kodlarÄ±

---

## Son SÃ¶z: Yolculuk Daha Yeni BaÅŸlÄ±yor

Bu yazÄ±da Ã§ok ÅŸey konuÅŸtuk: MDP'den Markov oyunlarÄ±na, Nash dengesinden best response'a, koordinasyon oyunlarÄ±ndan otonom araÃ§lara kadar.

Ama asÄ±l heyecan verici olan ÅŸu: MARL + Oyun Teorisi hala Ã§ok genÃ§ bir alan. BÃ¼yÃ¼k sorular cevaplanmayÄ± bekliyor. Yeni algoritmalar keÅŸfedilmeyi bekliyor. GerÃ§ek dÃ¼nya uygulamalarÄ± hayata geÃ§irilmeyi bekliyor.

Ve belki bu yazÄ±yÄ± okuyan sizlerden biri, o bÃ¼yÃ¼k sorularÄ±n cevabÄ±nÄ± bulacak. Belki bir yeni equilibrium konsepti geliÅŸtireceksiniz. Belki yÃ¼zlerce ajanÄ± koordine eden yeni bir Ã¶ÄŸrenme algoritmasÄ± tasarlayacaksÄ±nÄ±z.

Ben sadece kapÄ±yÄ± aralÄ±yorum. Ä°Ã§eri girmek size kalmÄ±ÅŸ.

---

**BeÄŸendiyseniz** ğŸ‘ **alkÄ±ÅŸlayÄ±n ve takip edin!** Ã–nÃ¼mÃ¼zdeki 51 hafta boyunca bu yolculukta birlikte olalÄ±m. SorularÄ±nÄ±z varsa yorumlarda buluÅŸalÄ±m.

*Bir sonraki yazÄ±da gÃ¶rÃ¼ÅŸmek Ã¼zere - Self-play'in bÃ¼yÃ¼lÃ¼ dÃ¼nyasÄ±nda!*

---

**GitHub'da Kod:** [github.com/highcansavci/marl-game-theory](https://github.com) â†’ TÃ¼m kod Ã¶rnekleri, notebook'lar ve ekstra materyaller

**Ä°letiÅŸim:** [highcsavci@gmail.com] - SorularÄ±nÄ±z, Ã¶nerileriniz her zaman hoÅŸ gelir
