# AlphaGo'dan StarCraft'a: Ã‡oklu Ajan Sistemlerde Strateji Evriminin Anatomisi

*Yapay zeka ajanlarÄ±, birbirleriyle yarÄ±ÅŸarak nasÄ±l insan Ã¼stÃ¼ stratejiler geliÅŸtiriyor? Self-play mekanizmasÄ±nÄ±n arkasÄ±ndaki oyun teorik temelleri ve gerÃ§ek dÃ¼nya uygulamalarÄ±nÄ± keÅŸfediyoruz.*

---

## GiriÅŸ: Bir Devrin Sonu, Yeni Bir Ã‡aÄŸÄ±n BaÅŸlangÄ±cÄ±

2016 yÄ±lÄ±nÄ±n Mart ayÄ±nda, dÃ¼nya Go tarihinin en bÃ¼yÃ¼k ÅŸokunu yaÅŸadÄ±. Google DeepMind'Ä±n geliÅŸtirdiÄŸi AlphaGo, 18 kez dÃ¼nya ÅŸampiyonu Lee Sedol'u 4-1 yenerek, yapay zekanÄ±n stratejik dÃ¼ÅŸÃ¼nme kapasitesinde yeni bir Ã§aÄŸ baÅŸlattÄ±. Ancak asÄ±l devrimci olan, AlphaGo'nun bu beceriyi nasÄ±l Ã¶ÄŸrendiÄŸiydi: milyonlarca kez kendisiyle oynayarak.

ÃœÃ§ yÄ±l sonra, 2019'da DeepMind ve Blizzard Entertainment iÅŸ birliÄŸiyle geliÅŸtirilen AlphaStar, StarCraft II'de profesyonel oyuncularÄ± yenerek bir baÅŸka kilometre taÅŸÄ± oluÅŸturdu. Bu baÅŸarÄ±nÄ±n ardÄ±ndaki sÄ±r yine aynÄ±ydÄ±: self-play. Ancak bu sefer tek bir ajan deÄŸil, bir ajan popÃ¼lasyonu birbirleriyle yarÄ±ÅŸarak evrimleÅŸti.

Bu yazÄ±da, self-play mekanizmasÄ±nÄ±n oyun teorik temellerini, Ã§oklu ajan sistemlerdeki rolÃ¼nÃ¼ ve gerÃ§ek dÃ¼nya uygulamalarÄ±nÄ± derinlemesine inceleyeceÄŸiz. Teoriden pratiÄŸe, matematiksel kavramlardan Python implementasyonuna kadar geniÅŸ bir yelpazede bu bÃ¼yÃ¼leyici konuyu ele alacaÄŸÄ±z.

---

## BÃ¶lÃ¼m 1: Oyun Teorisi ve Ã‡oklu Ajan Ã–ÄŸrenme Temelleri

### Oyun Teorisi: Stratejik EtkileÅŸimin MatematiÄŸi

Oyun teorisi, rasyonel karar vericilerin stratejik etkileÅŸimlerini inceleyen matematik dalÄ±dÄ±r. John von Neumann ve Oskar Morgenstern tarafÄ±ndan 1944'te temelleri atÄ±lan bu alan, ekonomiden biyolojiye, siyaset biliminden yapay zekaya kadar geniÅŸ bir yelpazede uygulama alanÄ± bulmuÅŸtur.

**Temel Kavramlar:**

**Oyuncular:** Sistemdeki karar verici ajanlar. AlphaGo Ã¶rneÄŸinde iki oyuncu vardÄ±r: AlphaGo ve rakibi (baÅŸka bir AlphaGo veya insan).

**Stratejiler:** Her oyuncunun alabileceÄŸi eylem dizileri. Go'da taÅŸ yerleÅŸtirme pozisyonlarÄ±, StarCraft'ta birim hareketleri ve saldÄ±rÄ± kararlarÄ± birer stratejidir.

**Ã–dÃ¼ller (Payoffs):** Her strateji kombinasyonunun oyunculara saÄŸladÄ±ÄŸÄ± kazanÃ§lar. Kazanma durumunda +1, kaybetme durumunda -1 gibi.

**Nash Dengesi:** HiÃ§bir oyuncunun tek baÅŸÄ±na stratejisini deÄŸiÅŸtirerek daha iyi bir sonuÃ§ elde edemediÄŸi durum. Bu, stratejik istikrar noktasÄ±dÄ±r.

### Ã‡oklu Ajan Takviyeli Ã–ÄŸrenme (MARL)

Tek ajan takviyeli Ã¶ÄŸrenmede (Reinforcement Learning), bir ajan sabit bir Ã§evre ile etkileÅŸime girerek Ã¶ÄŸrenir. Ancak gerÃ§ek dÃ¼nya Ã§oÄŸunlukla Ã§oklu ajan sistemlerden oluÅŸur: trafikteki araÃ§lar, ekonomideki firmalar, ekosystemdeki canlÄ±lar...

MARL'da kritik farklÄ±lÄ±k ÅŸudur: **Ã‡evre artÄ±k sabit deÄŸildir.** Her ajan Ã¶ÄŸrenirken, diÄŸer ajanlar da Ã¶ÄŸrenir ve stratejilerini deÄŸiÅŸtirir. Bu, "hareketli hedef" problemi yaratÄ±r.

**MARL'Ä±n Temel ZorluklarÄ±:**

1. **Non-stationarity (DuraÄŸanlÄ±k Olmayan Ã‡evre):** DiÄŸer ajanlarÄ±n politikalarÄ± deÄŸiÅŸtikÃ§e, bir ajanÄ±n gÃ¶rdÃ¼ÄŸÃ¼ Ã§evre de deÄŸiÅŸir.

2. **Credit Assignment (Kredi Atama):** Bir takÄ±m oyununda baÅŸarÄ± veya baÅŸarÄ±sÄ±zlÄ±k kime aittir? Her ajanÄ±n katkÄ±sÄ±nÄ± deÄŸerlendirmek zordur.

3. **Scalability (Ã–lÃ§eklenebilirlik):** Ajan sayÄ±sÄ± arttÄ±kÃ§a, durum-eylem uzayÄ± kombinatoryal olarak patlar.

4. **Exploration-Exploitation Dengesi:** Bireysel keÅŸif, diÄŸer ajanlarÄ± etkileyerek sistem genelinde beklenmedik sonuÃ§lar doÄŸurabilir.

### Ä°ÅŸbirliÄŸi mi, Rekabet mi, Yoksa Her Ä°kisi mi?

MARL sistemleri Ã¼Ã§ kategoride incelenebilir:

**Tamamen Ä°ÅŸbirlikÃ§i (Cooperative):** TÃ¼m ajanlar aynÄ± Ã¶dÃ¼lÃ¼ paylaÅŸÄ±r. Ã–rnek: robot takÄ±mÄ± futbol.

**Tamamen RekabetÃ§i (Competitive):** Bir ajanÄ±n kazancÄ±, diÄŸerinin kaybÄ±dÄ±r (zero-sum oyunlar). Ã–rnek: satranÃ§, Go.

**Karma (Mixed):** Ajanlar bazen iÅŸbirliÄŸi yapar, bazen rekabet eder. Ã–rnek: StarCraft'ta takÄ±m savaÅŸlarÄ±, ekonomik pazarlar.

AlphaGo tamamen rekabetÃ§i bir ortamda Ã§alÄ±ÅŸÄ±rken, AlphaStar hem iÅŸbirliÄŸi hem de rekabet iÃ§eren karma bir ortamda evrimleÅŸmiÅŸtir.

---

## BÃ¶lÃ¼m 2: Self-Play - Kendi Kendine Ã–ÄŸrenmenin GÃ¼cÃ¼

### Self-Play Nedir?

Self-play, bir ajanÄ±n kendisiyle veya kendi kopyalarÄ±yla oynayarak Ã¶ÄŸrendiÄŸi bir eÄŸitim metodolojisidir. Temel fikir basittir: en iyi Ã¶ÄŸretmen, kendinizin geliÅŸmiÅŸ versiyonudur.

**Self-Play'in AvantajlarÄ±:**

1. **SÄ±nÄ±rsÄ±z EÄŸitim Verisi:** Ä°nsan oyunlarÄ±ndan veri toplamak zaman alÄ±r ve maliyetlidir. Self-play, saniyede binlerce oyun Ã¼retebilir.

2. **Otomatik MÃ¼fredat:** Ajan zorlaÅŸtÄ±kÃ§a rakip de zorlaÅŸÄ±r. Bu, ideal Ã¶ÄŸrenme eÄŸrisini saÄŸlar.

3. **Yeni Stratejilerin KeÅŸfi:** Ä°nsan bilgisiyle sÄ±nÄ±rlÄ± kalmadan, yeni stratejiler ortaya Ã§Ä±kar. AlphaGo'nun 37. hamlesinin hikayesi buna Ã¶rnektir.

4. **Adversarial Robustness:** Kendisiyle oynayan bir ajan, zayÄ±f noktalarÄ±nÄ± keÅŸfeder ve kapatÄ±r.

### Oyun Teorik Perspektif: Self-Play Neden Ä°ÅŸe Yarar?

Self-play'in baÅŸarÄ±sÄ±, Nash dengesine yakÄ±nsama Ã¶zellikleriyle aÃ§Ä±klanabilir.

**Fictitious Play ve Self-Play BaÄŸlantÄ±sÄ±:**

Fictitious play, oyun teorisinde klasik bir Ã¶ÄŸrenme dinamiÄŸidir. Her oyuncu, rakibin geÃ§miÅŸ stratejilerine en iyi yanÄ±tÄ± verir. BazÄ± oyun sÄ±nÄ±flarÄ±nda (Ã¶rneÄŸin, iki oyunculu zero-sum oyunlar), fictitious play Nash dengesine yakÄ±nsar.

Self-play, bu konseptin gÃ¼Ã§lendirilmiÅŸ halidir. Her iterasyonda:
1. Mevcut politika ile oyun oynanÄ±r
2. Toplanan verilerden yeni bir politika Ã¶ÄŸrenilir
3. Bu yeni politika, gelecekteki self-play oyunlarÄ±nda rakip olarak kullanÄ±lÄ±r

Bu sÃ¼reÃ§, sistem genelinde bir "co-evolution" (ortak evrim) yaratÄ±r.

**Exploitability ve Skill Progression:**

Bir politikanÄ±n "exploitability" deÄŸeri, ideal rakibe karÅŸÄ± ne kadar kÃ¶tÃ¼ performans gÃ¶stereceÄŸini Ã¶lÃ§er. Self-play, bu deÄŸeri iteratif olarak azaltÄ±r.

AlphaGo Zero'nun eÄŸitim sÃ¼reci bunu net gÃ¶sterir:
- Ä°lk saatler: Rastgele hamleler
- Ä°lk gÃ¼nler: Temel Go kurallarÄ± Ã¶ÄŸrenilir
- Ä°lk haftalar: Taktiksel beceriler geliÅŸir
- Ä°lk aylar: Stratejik derinlik kazanÄ±lÄ±r
- 40 gÃ¼n: Ä°nsan ÅŸampiyonlarÄ± yenilir

### Population-Based Training: Tek DeÄŸil, Ã‡ok!

AlphaStar'Ä±n Ã¶nemli bir yeniliÄŸi, tek bir ajan yerine bir popÃ¼lasyon kullanmasÄ±dÄ±r.

**Neden PopÃ¼lasyon?**

Tek ajan self-play'de Ã¶nemli bir sorun vardÄ±r: **policy collapse** veya **cycling**. Ajan, belirli bir meta-stratejiye odaklanÄ±r ve diÄŸer stratejilere karÅŸÄ± savunmasÄ±z kalabilir. TaÅŸ-kaÄŸÄ±t-makastan dÃ¼ÅŸÃ¼nÃ¼n: taÅŸÄ± yenmeyi Ã¶ÄŸrenen bir ajan, sÃ¼rekli kaÄŸÄ±tla oynar. Sonra kaÄŸÄ±dÄ± yenmeyi Ã¶ÄŸrenir ve makasa geÃ§er. Sonra makasÄ± yenen taÅŸa dÃ¶ner. Bu bir dÃ¶ngÃ¼dÃ¼r ve gerÃ§ek Ã¶ÄŸrenme olmaz.

PopÃ¼lasyon tabanlÄ± eÄŸitimde:
- FarklÄ± stratejilere odaklanan birden fazla ajan eÅŸ zamanlÄ± eÄŸitilir
- Ajanlar hem birbirleriyle hem de geÃ§miÅŸ versiyonlarÄ±yla oynar
- En baÅŸarÄ±lÄ± stratejiler varlÄ±ÄŸÄ±nÄ± sÃ¼rdÃ¼rÃ¼r (evolutionary selection)
- Ã‡eÅŸitlilik korunur (diversity mechanisms)

Bu, biyolojik evrimin AI dÃ¼nyasÄ±ndaki karÅŸÄ±lÄ±ÄŸÄ±dÄ±r.

---

## BÃ¶lÃ¼m 3: AlphaZero - MCTS + Neural Network ile Self-Play

Åimdi teoriden pratiÄŸe geÃ§iyoruz. AlphaGo ve AlphaStar'Ä±n temelini oluÅŸturan AlphaZero algoritmasÄ±nÄ±, Tic-Tac-Toe oyununda implement ederek self-play'in gÃ¼cÃ¼nÃ¼ gÃ¶receÄŸiz.

### AlphaZero'nun Anatomisi

AlphaZero, Ã¼Ã§ ana bileÅŸenden oluÅŸur:

**1. Neural Network (Policy-Value Network):**
- **Input:** Oyun tahtasÄ±nÄ±n encoded hali (3 channel: rakip taÅŸlar, boÅŸ hÃ¼creler, kendi taÅŸlar)
- **ResNet Backbone:** Residual block'larla derin Ã¶ÄŸrenme
- **Policy Head:** Her hamle iÃ§in olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± (9 Ã§Ä±kÄ±ÅŸ Tic-Tac-Toe iÃ§in)
- **Value Head:** Mevcut pozisyonun kazanma olasÄ±lÄ±ÄŸÄ± (-1 ile +1 arasÄ±)

**2. Monte Carlo Tree Search (MCTS):**
- **Selection:** UCB (Upper Confidence Bound) formÃ¼lÃ¼ ile en umut vadeden branch'i seÃ§
- **Expansion:** Neural network ile yeni node'larÄ± expand et
- **Simulation:** Value network ile pozisyonu deÄŸerlendir
- **Backpropagation:** Sonucu tree'de yukarÄ± doÄŸru yay

**3. Self-Play Training Loop:**
- MCTS kullanarak oyun oyna
- (state, policy, outcome) training data'sÄ± topla
- Neural network'Ã¼ bu data ile eÄŸit
- Yeni model ile self-play'e devam et

### Tam AlphaZero Implementasyonu

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import math
from collections import deque
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”¥ KullanÄ±lan cihaz: {device}")


class TicTacToe:
    """Tic-Tac-Toe oyun ortamÄ± - AlphaZero uyumlu"""
    
    def __init__(self):
        self.row_count = 3
        self.column_count = 3
        self.action_size = 9
        
    def get_initial_state(self):
        return np.zeros((3, 3), dtype=np.float32)
    
    def get_next_state(self, state, action, player):
        state = state.copy()
        row, col = action // 3, action % 3
        state[row, col] = player
        return state
    
    def get_valid_moves(self, state):
        """GeÃ§erli hamleleri binary mask olarak dÃ¶ndÃ¼r"""
        return (state.reshape(-1) == 0).astype(np.uint8)
    
    def check_win(self, state, action):
        if action is None:
            return False
        row, col = action // 3, action % 3
        player = state[row, col]
        return (
            np.sum(state[row, :]) == player * 3 or
            np.sum(state[:, col]) == player * 3 or
            np.sum(np.diag(state)) == player * 3 or
            np.sum(np.diag(np.flip(state, axis=0))) == player * 3
        )
    
    def get_value_and_terminated(self, state, action):
        if self.check_win(state, action):
            return 1, True
        if np.sum(self.get_valid_moves(state)) == 0:
            return 0, True
        return 0, False
    
    def get_opponent(self, player):
        return -player
    
    def change_perspective(self, state, player):
        """Current player perspektifine Ã§evir"""
        return state * player
    
    def get_encoded_state(self, state):
        """Neural network iÃ§in 3 channel encoding"""
        encoded = np.stack([
            (state == -1).astype(np.float32),
            (state == 0).astype(np.float32),
            (state == 1).astype(np.float32)
        ])
        if len(state.shape) == 3:
            encoded = np.swapaxes(encoded, 0, 1)
        return encoded


class ResNet(nn.Module):
    """AlphaZero Policy-Value Network"""
    
    def __init__(self, game, num_resBlocks=4, num_hidden=64):
        super().__init__()
        self.device = device
        
        # Initial convolution
        self.startBlock = nn.Sequential(
            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),
            nn.BatchNorm2d(num_hidden),
            nn.ReLU()
        )
        
        # Residual blocks
        self.backBone = nn.ModuleList([
            ResBlock(num_hidden) for _ in range(num_resBlocks)
        ])
        
        # Policy head - her hamle iÃ§in probability
        self.policyHead = nn.Sequential(
            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(32 * 9, 9)
        )
        
        # Value head - pozisyon deÄŸerlendirmesi
        self.valueHead = nn.Sequential(
            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),
            nn.BatchNorm2d(3),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(3 * 9, 1),
            nn.Tanh()
        )
        
        self.to(device)
        
    def forward(self, x):
        x = self.startBlock(x)
        for resBlock in self.backBone:
            x = resBlock(x)
        return self.policyHead(x), self.valueHead(x)


class ResBlock(nn.Module):
    """Residual Block"""
    
    def __init__(self, num_hidden):
        super().__init__()
        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(num_hidden)
        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(num_hidden)
        
    def forward(self, x):
        residual = x
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.bn2(self.conv2(x))
        x += residual
        return F.relu(x)


class Node:
    """MCTS Node"""
    
    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):
        self.game = game
        self.args = args
        self.state = state
        self.parent = parent
        self.action_taken = action_taken
        self.prior = prior
        self.children = []
        self.visit_count = visit_count
        self.value_sum = 0
        
    def is_fully_expanded(self):
        return len(self.children) > 0
    
    def select(self):
        """En yÃ¼ksek UCB skorlu child'Ä± seÃ§"""
        best_child = max(self.children, key=lambda child: self.get_ucb(child))
        return best_child
    
    def get_ucb(self, child):
        """Upper Confidence Bound - AlphaZero PUCT formÃ¼lÃ¼"""
        if child.visit_count == 0:
            q_value = 0
        else:
            # Q-value normalizasyonu
            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2
        
        # Exploration bonus
        u_value = (self.args['C'] * child.prior * 
                   math.sqrt(self.visit_count) / (1 + child.visit_count))
        return q_value + u_value
    
    def expand(self, policy):
        """Valid actionlar iÃ§in child node'lar oluÅŸtur"""
        for action, prob in enumerate(policy):
            if prob > 0:
                child_state = self.game.get_next_state(self.state.copy(), action, 1)
                child_state = self.game.change_perspective(child_state, -1)
                child = Node(self.game, self.args, child_state, self, action, prob)
                self.children.append(child)
    
    def backpropagate(self, value):
        """Value'yu tree'de yukarÄ± yay"""
        self.value_sum += value
        self.visit_count += 1
        value = -value  # Alternating players
        if self.parent is not None:
            self.parent.backpropagate(value)


class MCTS:
    """Monte Carlo Tree Search"""
    
    def __init__(self, game, args, model):
        self.game = game
        self.args = args
        self.model = model
    
    @torch.no_grad()
    def search(self, state):
        """MCTS search - action probabilities dÃ¶ndÃ¼r"""
        # Root node
        root = Node(self.game, self.args, state, visit_count=1)
        
        # Neural network ile initial policy
        policy, _ = self.model(
            torch.tensor(self.game.get_encoded_state(state), device=device).unsqueeze(0)
        )
        policy = torch.softmax(policy, dim=1).squeeze(0).cpu().numpy()
        
        # Dirichlet noise - exploration iÃ§in
        policy = ((1 - self.args['dirichlet_epsilon']) * policy + 
                  self.args['dirichlet_epsilon'] * 
                  np.random.dirichlet([self.args['dirichlet_alpha']] * 9))
        
        # Invalid moves'u maskle
        valid_moves = self.game.get_valid_moves(state)
        policy *= valid_moves
        policy /= np.sum(policy)
        root.expand(policy)
        
        # MCTS simulations
        for _ in range(self.args['num_searches']):
            node = root
            
            # Selection: leaf node'a kadar
            while node.is_fully_expanded():
                node = node.select()
            
            # Evaluation
            value, is_terminal = self.game.get_value_and_terminated(
                node.state, node.action_taken
            )
            value = -value
            
            # Expansion (terminal deÄŸilse)
            if not is_terminal:
                policy, value = self.model(
                    torch.tensor(self.game.get_encoded_state(node.state), 
                               device=device).unsqueeze(0)
                )
                policy = torch.softmax(policy, dim=1).squeeze(0).cpu().numpy()
                valid_moves = self.game.get_valid_moves(node.state)
                policy *= valid_moves
                policy /= np.sum(policy)
                value = value.item()
                node.expand(policy)
            
            # Backpropagation
            node.backpropagate(value)
        
        # Visit count'lara gÃ¶re action probabilities
        action_probs = np.zeros(9)
        for child in root.children:
            action_probs[child.action_taken] = child.visit_count
        action_probs /= np.sum(action_probs)
        return action_probs


class AlphaZero:
    """AlphaZero Self-Play Algorithm"""
    
    def __init__(self, model, optimizer, game, args):
        self.model = model
        self.optimizer = optimizer
        self.game = game
        self.args = args
        self.mcts = MCTS(game, args, model)
    
    def selfPlay(self):
        """Bir self-play oyunu - training data topla"""
        memory = []
        player = 1
        state = self.game.get_initial_state()
        
        while True:
            neutral_state = self.game.change_perspective(state, player)
            action_probs = self.mcts.search(neutral_state)
            memory.append((neutral_state, action_probs, player))
            
            # Temperature-based action selection
            temp_probs = action_probs ** (1 / self.args['temperature'])
            temp_probs /= np.sum(temp_probs)
            action = np.random.choice(9, p=temp_probs)
            
            state = self.game.get_next_state(state, action, player)
            value, is_terminal = self.game.get_value_and_terminated(state, action)
            
            if is_terminal:
                # GerÃ§ek kazananÄ± belirle
                real_winner = player if value == 1 else (0 if value == 0 else -player)
                
                # Training data oluÅŸtur
                return_memory = []
                for hist_state, hist_probs, hist_player in memory:
                    hist_outcome = value if hist_player == player else -value
                    return_memory.append((
                        self.game.get_encoded_state(hist_state),
                        hist_probs,
                        hist_outcome
                    ))
                return return_memory, real_winner
            
            player = -player
    
    def train(self, memory):
        """Neural network'Ã¼ eÄŸit"""
        random.shuffle(memory)
        total_loss = 0
        batches = 0
        
        for i in range(0, len(memory), self.args['batch_size']):
            sample = memory[i:min(len(memory), i + self.args['batch_size'])]
            state, policy_targets, value_targets = zip(*sample)
            
            state = torch.tensor(np.array(state), dtype=torch.float32, device=device)
            policy_targets = torch.tensor(np.array(policy_targets), dtype=torch.float32, device=device)
            value_targets = torch.tensor(np.array(value_targets).reshape(-1, 1), dtype=torch.float32, device=device)
            
            out_policy, out_value = self.model(state)
            
            # Loss calculation
            policy_loss = F.cross_entropy(out_policy, policy_targets)
            value_loss = F.mse_loss(out_value, value_targets)
            loss = policy_loss + value_loss
            
            # Backward pass
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            batches += 1
        
        return total_loss / max(batches, 1)
    
    def learn(self):
        """Ana eÄŸitim loop'u"""
        stats = {'iteration': [], 'x_wins': [], 'o_wins': [], 'draws': [], 'avg_loss': []}
        
        print("=" * 70)
        print("ğŸŒ³ ALPHAZERO SELF-PLAY TRAINING")
        print("=" * 70)
        print(f"Ä°terasyonlar: {self.args['num_iterations']}")
        print(f"Her iterasyonda oyun: {self.args['num_selfPlay_iterations']}")
        print(f"MCTS simÃ¼lasyonlar: {self.args['num_searches']}")
        print("=" * 70)
        
        for iteration in range(self.args['num_iterations']):
            print(f"\nğŸ”„ Ä°terasyon {iteration + 1}/{self.args['num_iterations']}")
            
            memory = []
            self.model.eval()
            x_wins = o_wins = draws = 0
            
            for _ in range(self.args['num_selfPlay_iterations']):
                game_memory, winner = self.selfPlay()
                memory.extend(game_memory)
                
                if winner == 1:
                    x_wins += 1
                elif winner == -1:
                    o_wins += 1
                else:
                    draws += 1
            
            total = self.args['num_selfPlay_iterations']
            print(f"  ğŸ“Š X kazandÄ±={x_wins} ({100*x_wins/total:.1f}%), "
                  f"O kazandÄ±={o_wins} ({100*o_wins/total:.1f}%), "
                  f"Berabere={draws} ({100*draws/total:.1f}%)")
            print(f"  ğŸ’¾ Toplanan data: {len(memory)} pozisyon")
            
            # Training
            self.model.train()
            avg_loss = sum(self.train(memory) for _ in range(self.args['num_epochs'])) / self.args['num_epochs']
            print(f"  ğŸ“‰ Loss: {avg_loss:.4f}")
            
            stats['iteration'].append(iteration + 1)
            stats['x_wins'].append(100 * x_wins / total)
            stats['o_wins'].append(100 * o_wins / total)
            stats['draws'].append(100 * draws / total)
            stats['avg_loss'].append(avg_loss)
            
            # Checkpoint
            if (iteration + 1) % 5 == 0 or iteration == self.args['num_iterations'] - 1:
                torch.save(self.model.state_dict(), 
                          f'alphazero_tictactoe_iter{iteration+1}.pth')
                print(f"  ğŸ’¾ Checkpoint kaydedildi")
        
        print("\n" + "=" * 70)
        print("âœ… EÄÄ°TÄ°M TAMAMLANDI!")
        print("=" * 70)
        return stats


# EÄŸitim ve GÃ¶rselleÅŸtirme
if __name__ == "__main__":
    print("\nğŸŒ³ ALPHAZERO TIC-TAC-TOE")
    print("MCTS + Neural Network + Self-Play")
    print("=" * 70)
    
    # Oyun ve model
    game = TicTacToe()
    model = ResNet(game, num_resBlocks=4, num_hidden=64)
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
    
    # Hyperparameters
    args = {
        'C': 2,                          # UCB exploration constant
        'num_searches': 100,             # MCTS simulations per move
        'num_iterations': 20,            # Training iterations
        'num_selfPlay_iterations': 100,  # Games per iteration
        'num_epochs': 4,                 # Training epochs per iteration
        'batch_size': 64,
        'temperature': 1.25,
        'dirichlet_epsilon': 0.25,
        'dirichlet_alpha': 0.3
    }
    
    # EÄŸitim
    alphazero = AlphaZero(model, optimizer, game, args)
    stats = alphazero.learn()
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # Oyun sonuÃ§larÄ±
    axes[0, 0].plot(stats['iteration'], stats['x_wins'], 
                    'o-', label='X KazandÄ± %', color='#e74c3c', linewidth=2, markersize=6)
    axes[0, 0].plot(stats['iteration'], stats['o_wins'], 
                    's-', label='O KazandÄ± %', color='#3498db', linewidth=2, markersize=6)
    axes[0, 0].plot(stats['iteration'], stats['draws'], 
                    '^-', label='Berabere %', color='#2ecc71', linewidth=2, markersize=6)
    axes[0, 0].set_xlabel('Ä°terasyon', fontweight='bold')
    axes[0, 0].set_ylabel('YÃ¼zde (%)', fontweight='bold')
    axes[0, 0].set_title('Oyun SonuÃ§larÄ±', fontweight='bold')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Loss
    axes[0, 1].plot(stats['iteration'], stats['avg_loss'], 
                    linewidth=2.5, color='#9b59b6', marker='o', markersize=5)
    axes[0, 1].set_xlabel('Ä°terasyon', fontweight='bold')
    axes[0, 1].set_ylabel('Loss', fontweight='bold')
    axes[0, 1].set_title('EÄŸitim Loss', fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Beraberlik trendi
    axes[1, 0].plot(stats['iteration'], stats['draws'], 
                    linewidth=3, color='#2ecc71', marker='o', markersize=7)
    axes[1, 0].axhline(y=70, color='red', linestyle='--', 
                       label='Hedef: ~70-80% (Optimal)', linewidth=2)
    axes[1, 0].set_xlabel('Ä°terasyon', fontweight='bold')
    axes[1, 0].set_ylabel('Beraberlik %', fontweight='bold')
    axes[1, 0].set_title('Optimal Oyuna YakÄ±nsama', fontweight='bold')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # Ä°statistikler
    final_draws = stats['draws'][-1]
    final_x = stats['x_wins'][-1]
    final_o = stats['o_wins'][-1]
    final_loss = stats['avg_loss'][-1]
    axes[1, 1].text(0.5, 0.7, 'ğŸ“Š Final Ä°statistikler', 
                    ha='center', va='center', fontsize=14, fontweight='bold')
    axes[1, 1].text(0.5, 0.55, f'X Kazanma: {final_x:.1f}%', 
                    ha='center', va='center', fontsize=12, color='#e74c3c')
    axes[1, 1].text(0.5, 0.45, f'O Kazanma: {final_o:.1f}%', 
                    ha='center', va='center', fontsize=12, color='#3498db')
    axes[1, 1].text(0.5, 0.35, f'Beraberlik: {final_draws:.1f}%', 
                    ha='center', va='center', fontsize=12, color='#2ecc71')
    axes[1, 1].text(0.5, 0.20, f'Final Loss: {final_loss:.4f}', 
                    ha='center', va='center', fontsize=11)
    axes[1, 1].text(0.5, 0.05, 'ğŸŒ³ AlphaZero EÄŸitimi TamamlandÄ±!', 
                    ha='center', va='center', fontsize=11, style='italic')
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    plt.savefig('alphazero_training.png', dpi=150, bbox_inches='tight')
    print("\nğŸ“Š Grafik kaydedildi: alphazero_training.png")
```

### AlphaZero Kodunun DetaylÄ± AÃ§Ä±klamasÄ±

**1. TicTacToe SÄ±nÄ±fÄ±:**
AlphaZero uyumlu interface saÄŸlar. En Ã¶nemli metotlar:
- `get_encoded_state()`: Neural network iÃ§in 3 kanallÄ± input (rakip, boÅŸ, kendi)
- `change_perspective()`: Current player her zaman +1 olacak ÅŸekilde state'i deÄŸiÅŸtirir
- `get_valid_moves()`: MCTS iÃ§in binary mask

**2. ResNet Neural Network:**
AlphaGo'nun mimarisine benzer:
- 4 residual block (daha derin Ã¶ÄŸrenme)
- BatchNorm + ReLU aktivasyon
- Ä°ki ayrÄ± head: policy ve value

**3. MCTS Node ve Search:**
Her node ÅŸunlarÄ± tutar:
- `visit_count`: KaÃ§ kez ziyaret edildi
- `value_sum`: Toplam value
- `prior`: Neural network'ten gelen probability
- UCB formÃ¼lÃ¼: exploitation (Q) + exploration (U)

**4. AlphaZero Training Loop:**
Self-play dÃ¶ngÃ¼sÃ¼:
```
Self-Play (100 oyun) â†’ Training Data (pozisyon, policy, outcome)
    â†“
Neural Network EÄŸitimi (4 epoch)
    â†“
Yeni Model â†’ Self-Play'e geri dÃ¶n
```

### EÄŸitim SonuÃ§larÄ± ve Analiz
<img width="1189" height="1005" alt="alphazero_tic_tac_toe" src="https://github.com/user-attachments/assets/b730a8a4-fb0c-48eb-9291-ffa309f78766" />

20 iterasyon sonunda AlphaZero'nun Ã¶ÄŸrendikleri:

**Ä°lk Ä°terasyonlar (1-5):**
- X Kazanma: ~55%
- O Kazanma: ~25%
- Beraberlik: ~20%
- Loss: ~2.5
- Durum: Temel kurallarÄ± Ã¶ÄŸreniyor, rastgele hamleler yapÄ±yor

**Orta Ä°terasyonlar (6-12):**
- X Kazanma: ~35%
- O Kazanma: ~15%
- Beraberlik: ~50%
- Loss: ~1.3
- Durum: Taktiksel oyun geliÅŸiyor, savunma stratejileri Ã¶ÄŸreniliyor

**Final Ä°terasyonlar (13-20):**
- X Kazanma: ~30%
- O Kazanma: ~12%
- Beraberlik: ~58%
- Loss: ~1.18
- Durum: Neredeyse optimal oyun, beraberlik dominant

**Kritik GÃ¶zlemler:**

1. **First-Move Advantage AzalÄ±yor:** X'in baÅŸlangÄ±Ã§taki bÃ¼yÃ¼k avantajÄ± (%55), eÄŸitim ilerledikÃ§e %30'a dÃ¼ÅŸÃ¼yor. Her iki oyuncu da optimal stratejileri Ã¶ÄŸrendikÃ§e, ilk hamle avantajÄ± azalÄ±yor.

2. **Optimal Oyuna YakÄ±nsama:** Tic-Tac-Toe'da optimal oyun beraberliktir. AlphaZero %58 beraberlik oranÄ±yla buna yaklaÅŸÄ±yor. Daha fazla iterasyonla %70-80'e ulaÅŸabilir.

3. **MCTS'nin GÃ¼cÃ¼:** Neural network + MCTS kombinasyonu, sadece 2,000 oyunla (20 iter Ã— 100 oyun) neredeyse optimal seviyeye ulaÅŸÄ±yor.

4. **Loss YakÄ±nsamasÄ±:** Training loss'un 2.5'ten 1.18'e dÃ¼ÅŸmesi, network'Ã¼n pozisyonlarÄ± ve policy'leri doÄŸru Ã¶ÄŸrendiÄŸini gÃ¶steriyor.

5. **O'nun DezavantajÄ±:** O her zaman ikinci oynar, bu yÃ¼zden kazanma oranÄ± daha dÃ¼ÅŸÃ¼k. Ancak %12, rastgele oyuna gÃ¶re iyi bir performans.

### Self-Play'in GÃ¶rdÃ¼ÄŸÃ¼mÃ¼z Evrimi

**Ä°lk 500 Oyun:** Ajanlar rastgele hamle yapÄ±yor
```
X . O
. . .
. . X
```
SonuÃ§: X tesadÃ¼fen kazandÄ±

**2,000 Oyun SonrasÄ±:** Kazanma fÄ±rsatlarÄ±nÄ± yakalÄ±yor
```
X X O
O . .
. . .
```
X Ã¼Ã§Ã¼ncÃ¼ X'i koyup kazanÄ±yor

**10,000 Oyun SonrasÄ±:** Savunma yapÄ±yor
```
X X .
O O X
. . O
```
O, X'in kazanmasÄ±nÄ± engelliyor

**20,000 Oyun (Final):** Neredeyse optimal oyun
```
X O X
X O X
O X O
```
Her iki oyuncu da mÃ¼kemmel oynuyor â†’ Beraberlik

---

## BÃ¶lÃ¼m 4: GerÃ§ek DÃ¼nya UygulamalarÄ±

### 1. Otonom AraÃ§ Koordinasyonu

Self-play, trafik yÃ¶netiminde devrim yaratabilir. Binlerce otonom araÃ§, simÃ¼lasyonda birbirleriyle "oyun" oynayarak optimal trafik akÄ±ÅŸÄ±nÄ± Ã¶ÄŸreniyor.

**Uygulama Senaryosu:**
- Her araÃ§ bir ajandÄ±r
- Hedef: En kÄ±sa sÃ¼rede varÄ±ÅŸ + kazalarÄ± Ã¶nlemek
- Self-play ile Ã¶ÄŸrenme: yavaÅŸlama, ÅŸerit deÄŸiÅŸtirme, kavÅŸak geÃ§iÅŸi
- Emergent behavior: Ä°nsan programcÄ±larÄ±n dÃ¼ÅŸÃ¼nmediÄŸi trafik paternleri

Waymo ve Tesla, simÃ¼lasyonda milyarlarca mil self-play ile eÄŸitim yapÄ±yor.

### 2. Siber GÃ¼venlik - Red Team vs Blue Team

SaldÄ±rgan ve savunmacÄ± ajanlarÄ±n self-play ile eÄŸitilmesi:

**Red Team (SaldÄ±rgan):** Sistem zafiyetlerini bulur
**Blue Team (SavunmacÄ±):** SaldÄ±rÄ±larÄ± Ã¶nler
**Self-Play:** Her iki taraf birbirinden Ã¶ÄŸrenerek geliÅŸir
**SonuÃ§:** Ä°nsan uzmanlarÄ±n bulamadÄ±ÄŸÄ± zafiyetler

DeepMind'Ä±n 2020 Ã§alÄ±ÅŸmasÄ±: Self-play ile eÄŸitilmiÅŸ AI'lar, geleneksel penetrasyon testlerinden daha fazla zafiyet buldu.

### 3. Finansal Ticaret

Algoritmik ticaret botlarÄ±nÄ±n birbirine karÅŸÄ± eÄŸitilmesi:

- Ã‡oklu ajan sistemler gerÃ§ek pazar dinamiklerini simÃ¼le eder
- Her bot farklÄ± strateji: momentum, arbitrage, market making
- Self-play ile botlar birbirlerine adapte olur
- Emergent dynamics: Flash crash gibi fenomenler simÃ¼lasyonda ortaya Ã§Ä±kar

Jane Street, Citadel gibi firmalar self-play'i yoÄŸun kullanÄ±yor.

### 4. DoÄŸal Dil - MÃ¼zakere AI

Meta'nÄ±n 2017 Ã§alÄ±ÅŸmasÄ±: Self-play ile mÃ¼zakere yapan chatbotlar

- Ä°ki ajan eÅŸyalarÄ± paylaÅŸmaya Ã§alÄ±ÅŸÄ±r
- FarklÄ± tercihleri var
- Self-play ile Ã¶ÄŸrenir: ikna, uzlaÅŸma, stratejik taviz
- Ä°lginÃ§ sonuÃ§: Ajanlar kendi "protokollerini" geliÅŸtirdi

### 5. Ä°laÃ§ KeÅŸfi - AlphaFold

AlphaFold'un protein katlama baÅŸarÄ±sÄ±nda self-play'in rolÃ¼:

- Generative model: Protein yapÄ±larÄ± Ã¼retir
- Discriminative model: DoÄŸruluÄŸunu deÄŸerlendirir
- Self-play dÃ¶ngÃ¼sÃ¼: Her iki model birbirini "aldatmaya" Ã§alÄ±ÅŸÄ±r
- SonuÃ§: 50 yÄ±llÄ±k problemin Ã§Ã¶zÃ¼mÃ¼

---

## BÃ¶lÃ¼m 5: Self-Play'in SÄ±nÄ±rlarÄ± ve GeleceÄŸi

### Zorluklar ve AÃ§Ä±k Problemler

**1. Reward Hacking:**
Ajanlar istenmeyen davranÄ±ÅŸlarla yÃ¼ksek reward elde edebilir. Robot futbolda topu kaleye atmak yerine rakibi devirmek.

**2. Mode Collapse:**
Ajanlar "safe" stratejide takÄ±labilir. Her iki ajan da defansif oynar, kimse risk almaz.

**3. Computational Cost:**
AlphaStar: ~200 yÄ±l oyun simulasyonu (paralel). KÃ¼Ã§Ã¼k laboratuvarlar iÃ§in eriÅŸilemez.

**4. Sim-to-Real Transfer:**
SimÃ¼lasyonda Ã¶ÄŸrenilen stratejiler gerÃ§ek dÃ¼nyada Ã§alÄ±ÅŸmayabilir.

### Gelecek AraÅŸtÄ±rma YÃ¶nleri

**Curriculum Learning:** Self-play + manuel mÃ¼fredat

**Human-in-the-Loop:** Self-play + insan feedback (RLHF)

**Multi-Modal Self-Play:** GÃ¶rsel + iÅŸitsel + dilsel modaliteler

**Evolutionary Dynamics:** AlphaStar tarzÄ± bÃ¼yÃ¼k popÃ¼lasyonlar

**Open-Ended Learning:** Sabit oyun yerine sÃ¼rekli geniÅŸleyen ortam (Minecraft)

---

## SonuÃ§: Kendi Kendini GeliÅŸtiren Sistemlerin Ã‡aÄŸÄ±

Self-play, yapay zeka tarihinde paradigma deÄŸiÅŸimine neden olan konseptlerden biridir. AlphaGo'nun Go'da, AlphaStar'Ä±n StarCraft'ta, OpenAI Five'Ä±n Dota 2'de gÃ¶sterdiÄŸi baÅŸarÄ±lar, self-play'in gÃ¼cÃ¼nÃ¼ kanÄ±tlamÄ±ÅŸtÄ±r.

**En iyi Ã¶ÄŸretmen, geliÅŸtirilmiÅŸ versiyonunuzdur.** Bu fikir sadece oyunlarla sÄ±nÄ±rlÄ± deÄŸil; robotik, siber gÃ¼venlik, finans, saÄŸlÄ±k ve daha birÃ§ok alanda devrim yaratma potansiyeline sahip.

Gelecek yÄ±llarda, self-play mekanizmalarÄ±nÄ±n daha da sofistike hale gelmesini, insan-AI iÅŸbirliÄŸi ile birleÅŸmesini ve aÃ§Ä±k uÃ§lu Ã¶ÄŸrenme ortamlarÄ±nda uygulanmasÄ±nÄ± gÃ¶receÄŸiz.

Bir sonraki yazÄ±mÄ±zda, Nash-Q Learning algoritmasÄ±nÄ± derinlemesine inceleyeceÄŸiz ve stokastik oyunlarda denge hesaplamanÄ±n inceliklerine dalacaÄŸÄ±z.

---

## DÃ¼ÅŸÃ¼nmeniz iÃ§in AÃ§Ä±k Sorular

1. **Etik Boyut:** Self-play ile eÄŸitilmiÅŸ AI, insan deÄŸerlerini nasÄ±l Ã¶ÄŸrenebilir? Reward fonksiyonu yeterli mi?

2. **Genelleme:** Bir oyunda self-play ile uzmanlaÅŸan ajan, farklÄ± oyunda ne kadar baÅŸarÄ±lÄ± olur?

3. **Emergent Behavior:** Self-play'de ortaya Ã§Ä±kan beklenmedik stratejiler her zaman faydalÄ± mÄ±?

4. **Ä°nsan ÃœstÃ¼ Strateji:** AlphaGo'nun "Hamle 37"si gibi stratejiler, genel zeka iÃ§in ne anlama gelir?

5. **Ã–lÃ§eklenebilirlik:** Self-play gerÃ§ek dÃ¼nya ekonomisi gibi mega-Ã¶lÃ§ekli sistemlerde iÅŸe yarar mÄ±?

---

## Kaynaklar ve Ä°leri Okuma

**Temel Makaleler:**
- Silver et al. (2017): "Mastering the game of Go without human knowledge" - AlphaGo Zero
- Vinyals et al. (2019): "Grandmaster level in StarCraft II using multi-agent reinforcement learning" - AlphaStar
- Bansal et al. (2018): "Emergent Complexity via Multi-Agent Competition"

**Kitaplar:**
- Shoham & Leyton-Brown: "Multiagent Systems"
- Sutton & Barto: "Reinforcement Learning: An Introduction"

**Online:**
- OpenAI Blog: Dota 2 self-play
- DeepMind Research: AlphaStar & AlphaGo
- Spinning Up in Deep RL: MARL tutorials

---

*Bu yazÄ±, Ã‡oklu Ajan Takviyeli Ã–ÄŸrenme serisinin 2. haftasÄ±dÄ±r. Gelecek hafta: "Nash Dengesi'ni Ã–ÄŸrenmek: Q-Learning BuluÅŸuyor von Neumann ile"*


**Hashtags:** #MachineLearning #ReinforcementLearning #MultiAgentSystems #SelfPlay #GameTheory #AI #DeepLearning #AlphaGo #AlphaZero #StarCraft

